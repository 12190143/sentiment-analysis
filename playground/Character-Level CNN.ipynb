{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import utils.ymr_data as ymr\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import math\n",
    "import time\n",
    "import itertools\n",
    "import pickle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "SENTENCE_LENGTH_PADDED=512\n",
    "EMBEDDING_SIZE = 150\n",
    "BATCH_SIZE=128\n",
    "EVALUATE_DEV_EVERY=16\n",
    "L1_NUM_FILTERS = 150\n",
    "L1_FILTER_SIZES = [2,3,4]\n",
    "CHECKPOINTS_DIR = \"./checkpoints/\"\n",
    "TRAIN_SUMMARY_DIR = \"./summaries/train\"\n",
    "DEV_SUMMARY_DIR = \"./summaries/dev\"\n",
    "\n",
    "PADDING_CHARACTER =  u\"\\u0000\"\n",
    "NUM_CLASSES=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 29017\n",
      "Dev set size: 1528\n",
      "Test set size: 7637\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = ymr.load()\n",
    "\n",
    "# Preprocessing: Pad all sentences\n",
    "df.text = df.text.str.slice(0,SENTENCE_LENGTH_PADDED).str.ljust(SENTENCE_LENGTH_PADDED, PADDING_CHARACTER)\n",
    "\n",
    "# Generate vocabulary and dataset\n",
    "vocab, vocab_inv = ymr.vocab(df)\n",
    "data = ymr.make_polar(df)\n",
    "train, test = ymr.train_test_split(data)\n",
    "train_x, train_y_ = ymr.make_xy(train, vocab)\n",
    "test_x, test_y_ = ymr.make_xy(test, vocab)\n",
    "\n",
    "VOCABULARY_SIZE = len(vocab)\n",
    "\n",
    "# Convert ys to probability distribution\n",
    "train_y = np.zeros((len(train_y_), NUM_CLASSES))\n",
    "train_y[np.arange(len(train_y_)), train_y_] = 1.\n",
    "test_y = np.zeros((len(test_y_), NUM_CLASSES))\n",
    "test_y[np.arange(len(test_y_)), test_y_] = 1.\n",
    "\n",
    "# Use a dev set\n",
    "train_x, dev_x, train_y, dev_y = train_test_split(train_x, train_y, test_size=0.05)\n",
    "\n",
    "print(\"Training set size: %d\" % len(train_y))\n",
    "print(\"Dev set size: %d\" % len(dev_y))\n",
    "print(\"Test set size: %d\" % len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CharCNN:\n",
    "    def __init__(self, vocabulary_size, embedding_size=128, filters_sizes=[1, 2, 3], num_filters=128,\n",
    "                affine_dim=256, dropout_keep_prob=0.5, num_gpus=1, optimizer=None):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.filters_sizes = filters_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.affine_dim = affine_dim\n",
    "        self.dropout_keep_prob = dropout_keep_prob\n",
    "        self.num_gpus = num_gpus\n",
    "        self.optimizer = optimizer if optimizer else tf.train.AdamOptimizer(1e-4)\n",
    "        # Assigned when building the graph\n",
    "        self.tensors = {}\n",
    "\n",
    "    def build_input_batches(self, train_x, train_y, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE):\n",
    "        \"\"\"\n",
    "        Builds a graph that stores the input in memory and uses queues\n",
    "        to slice it into bactches.\n",
    "\n",
    "        Returns a node representing batches of x and y.\n",
    "        \"\"\"\n",
    "        # Store the data in graph notes\n",
    "        train_x_const = tf.constant(train_x.astype(\"int32\"))\n",
    "        train_y_const = tf.constant(train_y.astype(\"float32\"))\n",
    "        # Use Tensorflow's queues and batching features\n",
    "        x_slice, y_slice = tf.train.slice_input_producer([train_x_const, train_y_const], num_epochs=num_epochs)\n",
    "        x, y = tf.train.batch([x_slice, y_slice], batch_size=BATCH_SIZE)\n",
    "        return [x, y]\n",
    "\n",
    "    def build_embedding_layer(self, shape, input_tensor):\n",
    "        \"\"\"\n",
    "        Builds an embedding layer.\n",
    "\n",
    "        Returns the final embedding.\n",
    "        \"\"\"\n",
    "        # We force this on the CPU because the op isn't implemented for the GPU yet\n",
    "        with tf.device('/cpu:0'):\n",
    "            W_intializer = tf.random_uniform(shape, -1.0, 1.0)\n",
    "            W_embeddings = tf.Variable(W_intializer, name=\"W\")\n",
    "            return tf.nn.embedding_lookup(W_embeddings, input_tensor)\n",
    "\n",
    "    def build_conv_maxpool(self, filter_shape, pool_shape, input_tensor):\n",
    "        \"\"\"\n",
    "        Builds a convolutional layer followed by a max-pooling layer.\n",
    "        \"\"\"\n",
    "        W = tf.get_variable(\"W\", filter_shape, initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b = tf.get_variable(\"b\", filter_shape[-1], initializer=tf.constant_initializer(0.1))\n",
    "        conv = tf.nn.conv2d(input_tensor, W, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "        h = tf.nn.relu(conv + b, name=\"conv\")\n",
    "        return tf.nn.max_pool(h, ksize=pool_shape, strides=[1, 1, 1, 1], padding='VALID', name=\"pool\")\n",
    "\n",
    "    def build_affine(self, shape, input_tensor):\n",
    "        \"\"\"\n",
    "        Builds an affine (fully-connected) layer\n",
    "        \"\"\"\n",
    "        W = tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=shape[-1:]), name=\"b\")\n",
    "        h = tf.nn.relu(tf.matmul(input_tensor, W) + b, name=\"h\")\n",
    "        return h\n",
    "\n",
    "    def build_softmax(self, shape, input_tensor):\n",
    "        \"\"\"\n",
    "        Builds a softmax layer\n",
    "        \"\"\"\n",
    "        W = tf.Variable(tf.truncated_normal(shape, stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=shape[-1:]), name=\"b\")\n",
    "        return tf.nn.softmax(tf.matmul(input_tensor, W) + b, name=\"y\")\n",
    "\n",
    "    def inference(self, x, labels):\n",
    "        \"\"\"\n",
    "        Builds the graph and returns the final prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        sequence_length = x.get_shape().as_list()[1]\n",
    "        num_classes = labels.get_shape().as_list()[1]\n",
    "\n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            embedded_chars = self.build_embedding_layer([self.vocabulary_size, self.embedding_size], x)\n",
    "            embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(self.filters_sizes):\n",
    "            with tf.variable_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                with tf.device(\"/gpu:%d\" % (i % self.num_gpus)):\n",
    "                    filter_shape = [filter_size, self.embedding_size, 1, self.num_filters]\n",
    "                    pool_shape = [1, sequence_length - filter_size + 1, 1, 1]\n",
    "                    pooled = self.build_conv_maxpool(filter_shape, pool_shape, embedded_chars_expanded)\n",
    "                    pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = self.num_filters * len(self.filters_sizes)\n",
    "        h_pool = tf.concat(3, pooled_outputs)\n",
    "        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Affine Layer with dropout\n",
    "        with tf.variable_scope(\"affine\"):\n",
    "            h_affine = self.build_affine([num_filters_total, self.affine_dim], h_pool_flat)\n",
    "        h_affine_drop = tf.nn.dropout(h_affine, self.dropout_keep_prob)\n",
    "\n",
    "        # Softmax Layer (Final output)\n",
    "        with tf.variable_scope(\"softmax\"):\n",
    "            y = self.build_softmax([self.affine_dim, num_classes], h_affine_drop)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    def loss(self, predictions, labels):\n",
    "        \"\"\"\n",
    "        Calculates the mean cross-entropy loss\n",
    "        \"\"\"\n",
    "        return -tf.reduce_mean(labels * tf.log(predictions), name=\"loss\")\n",
    "    \n",
    "    def accuracy(self, y, labels):\n",
    "        \"\"\"\n",
    "        Returns accuracy tensor\n",
    "        \"\"\"\n",
    "        correct_predictions = tf.equal(tf.argmax(y, 1), tf.argmax(labels, 1), name=\"correct_predictions\")\n",
    "        return tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "\n",
    "    def train(self, loss, global_step):\n",
    "        \"\"\"\n",
    "        Returns train op\n",
    "        \"\"\"\n",
    "        return self.optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    def print_parameters(self):\n",
    "        print \"\\nParameters:\"\n",
    "        print(\"----------\")\n",
    "        total_parameters = 0\n",
    "        for v in tf.trainable_variables():\n",
    "            num_parameters = v.get_shape().num_elements()\n",
    "            print(\"{}: {:,}\".format(v.name, num_parameters))\n",
    "            total_parameters += num_parameters\n",
    "        print(\"Total Parameters: {:,}\\n\".format(total_parameters))      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GraphSerDe:\n",
    "    \n",
    "    def serialize(self, dirname, graph_def):\n",
    "        graph_file = os.path.join(dirname, \"graph.pb\")\n",
    "        varfile = os.path.join(dirname, \"variables.pk1\")\n",
    "        # Write graph\n",
    "        tf.train.write_graph(graph_def, dirname, \"graph.pb\", as_text=False)\n",
    "        print(\"Wrote GraphDef to {}\".format(graph_file))\n",
    "        # Write variables\n",
    "        var_names = [v.name for v in tf.all_variables()]\n",
    "        with open(varfile, \"wb\") as f:\n",
    "            pickle.dump(var_names, f)\n",
    "        print(\"Wrote variables to {}\".format(varfile))\n",
    "    \n",
    "    def deserialize(self, dirname):\n",
    "        graph_file = os.path.join(dirname, \"graph.pb\")\n",
    "        varfile = os.path.join(dirname, \"variables.pk1\")\n",
    "        with open(graph_file, \"rb\") as f:\n",
    "            graph_def = tf.GraphDef.FromString(f.read())\n",
    "        with open(varfile, \"rb\") as f:\n",
    "            var_names = pickle.load(f)\n",
    "        return [graph_def, var_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/5z/1fqtfnl112q6_595rkrfh6z80000gn/T/tmpzL0GS4\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'W:0']\n",
      "Wrote GraphDef to /var/folders/5z/1fqtfnl112q6_595rkrfh6z80000gn/T/tmpBvi89g-graph/graph.pb\n",
      "Wrote variables to /var/folders/5z/1fqtfnl112q6_595rkrfh6z80000gn/T/tmpBvi89g-graph/variables.pk1\n",
      "[]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-287-f1ca611dca96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_elements\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"import/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "tmpdir =  tempfile.mkdtemp() + \"-graph\"\n",
    "with tf.Graph().as_default():\n",
    "    a = tf.Variable(tf.truncated_normal([5,5], stddev=0.1), name=\"W\")\n",
    "    print [v.name for v in tf.all_variables()]\n",
    "    serde = GraphSerDe()\n",
    "    ser.serialize(tmpdir, tf.get_default_graph().as_graph_def())\n",
    "    \n",
    "with tf.Graph().as_default():\n",
    "    print [v.name for v in tf.all_variables()]\n",
    "    serde = GraphSerDe()\n",
    "    graph_def, var_names = serde.deserialize(tmpdir)\n",
    "    variables = tf.import_graph_def(graph_def, return_elements=var_names)\n",
    "    for v in variables:\n",
    "        v.name = v.name.replace(\"import/\", \"\")\n",
    "    print [v.name for v in variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, train_op, global_step, summary_op, eval_feed_dict,\n",
    "                 save_every=10, evaluate_every=10):\n",
    "        self.train_op = train_op\n",
    "        self.global_step = global_step\n",
    "        self.summary_op = summary_op\n",
    "        self.save_every = save_every\n",
    "        self.evaluate_every = evaluate_every\n",
    "        self.eval_feed_dict = eval_feed_dict\n",
    "        \n",
    "        # All data goes in here\n",
    "        RUNDIR = \"./runs/%s\" % int(time.time())\n",
    "        \n",
    "        # Write graph\n",
    "        # gsd = GraphSerDe()\n",
    "        # gsd.serialize(\"%s/graph\" % RUNDIR)      \n",
    "        \n",
    "        # Initialize summary writers\n",
    "        self.train_writer = tf.train.SummaryWriter(\"%s/summaries/train\" % RUNDIR)\n",
    "        self.eval_writer = tf.train.SummaryWriter(\"%s/summaries/eval\" % RUNDIR)\n",
    "        \n",
    "        # Initialize saver\n",
    "        self.save_prefix = \"%s/checkpoints/model\" % RUNDIR\n",
    "        if not os.path.exists(os.path.dirname(self.save_prefix)):\n",
    "            os.makedirs(os.path.dirname(self.save_prefix))    \n",
    "        self.saver = tf.train.Saver(tf.all_variables())        \n",
    "    \n",
    "    def evaluate(self):\n",
    "        summaries_, global_step_ = sess.run([summary_op,self.global_step], feed_dict=self.eval_feed_dict)\n",
    "        self.eval_writer.add_summary(summaries_, global_step_)\n",
    "        # Print summaries\n",
    "        print(\"\\n========== Evaluation ==========\")\n",
    "        summary_obj = tf.Summary.FromString(summaries_)\n",
    "        interesting_summaries = [v for v in summary_obj.value if not \"queue/\" in v.tag]\n",
    "        print \"\\n\".join([\"{}: {:f}\".format(v.tag, v.simple_value) for v in interesting_summaries])\n",
    "        print(\"\")\n",
    "    \n",
    "    def step(self):\n",
    "        sess = tf.get_default_session()\n",
    "        # Run training step\n",
    "        _, global_step_, summaries_ = sess.run([self.train_op, self.global_step, self.summary_op])\n",
    "        print(\"{}: Step {}\".format(datetime.datetime.now().isoformat(), global_step_))\n",
    "        # Write summary\n",
    "        self.train_writer.add_summary(summaries_, global_step_)\n",
    "        # Maybe save\n",
    "        if global_step_ % SAVE_EVERY == 0:\n",
    "            save_path = saver.save(sess, self.save_prefix, global_step_)\n",
    "            print(\"\\nSaved model parameters to %s\" % save_path)\n",
    "        # Maybe evaluate\n",
    "        if global_step_ % EVALUATE_EVERY == 0:\n",
    "            self.evaluate()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-11-21T00:53:03.190714: Step 1\n",
      "2015-11-21T00:53:05.248097: Step 2\n",
      "2015-11-21T00:53:07.203884: Step 3\n",
      "2015-11-21T00:53:09.140016: Step 4\n",
      "2015-11-21T00:53:11.096717: Step 5\n",
      "\n",
      "Saved model to ./runs/1448063580/checkpoints/model-5\n",
      "\n",
      "========== Evaluation ==========\n",
      "loss: 0.753661\n",
      "accuracy: 0.263743\n",
      "\n",
      "2015-11-21T00:53:23.602652: Step 6\n",
      "2015-11-21T00:53:25.711613: Step 7\n",
      "2015-11-21T00:53:27.784368: Step 8\n",
      "2015-11-21T00:53:29.752793: Step 9\n",
      "2015-11-21T00:53:31.696584: Step 10\n",
      "\n",
      "Saved model to ./runs/1448063580/checkpoints/model-10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-294-7fe2726d5bbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-293-1c7343e3e248>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Maybe evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mglobal_step_\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mEVALUATE_EVERY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-293-1c7343e3e248>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0msummaries_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummaries_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Print summaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/projects/python-venvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/projects/python-venvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 404\u001b[0;31m                                target_list)\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    cnn = CharCNN(VOCABULARY_SIZE)\n",
    "    \n",
    "    # Generate input batches\n",
    "    with tf.variable_scope(\"input\"):\n",
    "        x, labels = cnn.build_input_batches(train_x, train_y)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = cnn.inference(x, labels)\n",
    "    \n",
    "    # Loss\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        loss = cnn.loss(predictions, labels)\n",
    "    \n",
    "    # Train\n",
    "    global_step = tf.Variable(0, name=\"global_step\")\n",
    "    train_op = cnn.train(loss, global_step)\n",
    "    \n",
    "    # Summaries\n",
    "    tf.scalar_summary(\"loss\", loss)\n",
    "    tf.scalar_summary(\"accuracy\", cnn.accuracy(predictions, labels))\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    eval_feed_dict = { x: dev_x, labels: dev_y }\n",
    "    trainer = Trainer(train_op, global_step, summary_op, eval_feed_dict)\n",
    "    \n",
    "    # Create an run session\n",
    "    step = 0\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    with sess.as_default():\n",
    "        # Initialize\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                trainer.step()\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Done!\")\n",
    "        finally:\n",
    "            coord.request_stop()      \n",
    "        coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
