{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn import rnn as tf_rnn\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.pardir)\n",
    "from utils.mixins import NNMixin, TrainMixin\n",
    "from utils import ymr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Model Hyperparameters\n",
    "SENTENCE_LENGTH_PADDED = NUM_STEPS = 256\n",
    "HIDDEN_DIM = 128\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "# Training parameters\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "EVALUATE_EVERY = 16\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train/dev/test size: 29017/1528/7637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, dev_x, dev_y, test_x, test_y = ymr_data.generate_dataset(fixed_length=SENTENCE_LENGTH_PADDED)\n",
    "VOCABULARY_SIZE = max(train_x.max(), dev_x.max(), test_x.max()) + 1\n",
    "print(\"\\ntrain/dev/test size: {:d}/{:d}/{:d}\\n\".format(len(train_y), len(dev_y), len(test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CharRNN(object, NNMixin, TrainMixin):\n",
    "    def __init__(\n",
    "      self, vocabulary_size, sequence_length, batch_size, num_classes,\n",
    "      embedding_size=128, hidden_dim=256, cell=None, num_layers=3, loss=\"linear_gain\"):\n",
    "\n",
    "        self.input_x = tf.placeholder(tf.int32, [batch_size, sequence_length])\n",
    "        self.input_y = tf.placeholder(tf.float32, [batch_size, num_classes])\n",
    "\n",
    "        if not cell:\n",
    "            # Standard cell: Stacked LSTM\n",
    "            first_cell = rnn_cell.BasicLSTMCell(hidden_dim)\n",
    "            # first_cell = rnn_cell.LSTMCell(hidden_dim, embedding_size, use_peepholes=True)\n",
    "            # next_cell = rnn_cell.LSTMCell(hidden_dim, hidden_dim, use_peepholes=True)\n",
    "            # self.cell = rnn_cell.MultiRNNCell([first_cell] + [next_cell] * (num_layers - 1))\n",
    "            self.cell = first_cell\n",
    "\n",
    "        with tf.variable_scope(\"embedding\"):\n",
    "            self.embedded_chars = self._build_embedding([vocabulary_size, embedding_size], self.input_x)\n",
    "\n",
    "        with tf.variable_scope(\"rnn\") as scope:\n",
    "            self.initial_state = tf.placeholder(tf.float32, [batch_size, self.cell.state_size])\n",
    "            # self.initial_state = tf.Variable(tf.zeros([batch_size, self.cell.state_size]))\n",
    "#             self.outputs = []\n",
    "#             self.states = [self.initial_state]\n",
    "#             for i in range(sequence_length):\n",
    "#                 if i > 0:\n",
    "#                     scope.reuse_variables()\n",
    "#                 new_output, new_state = self.cell(self.embedded_chars[:, i, :], self.states[-1])\n",
    "#                 self.outputs.append(new_output)\n",
    "#                 self.states.append(new_state)\n",
    "#             self.final_state = self.states[-1]\n",
    "#             self.final_output = self.outputs[-1]\n",
    "            item_list = [tf.squeeze(x) for x in tf.split(1, sequence_length, self.embedded_chars)]\n",
    "            self.outputs, self.states = tf_rnn.rnn(self.cell, item_list, initial_state=self.initial_state)\n",
    "            self.final_state = self.states[-1]\n",
    "            self.final_output = self.outputs[-1]\n",
    "\n",
    "        with tf.variable_scope(\"softmax\") as scope:\n",
    "            self.ys = []\n",
    "            for i, o in enumerate(self.outputs):\n",
    "                if i > 0:\n",
    "                    scope.reuse_variables()\n",
    "                y = self._build_softmax([hidden_dim, num_classes], o)\n",
    "                self.ys.append(y)\n",
    "            self.y = self.ys[-1]\n",
    "            self.predictions = tf.argmax(self.y, 1)\n",
    "\n",
    "        with tf.variable_scope(\"loss\"):\n",
    "#           if loss == \"linear_gain\":\n",
    "            # Loss with linear gain. We output at each time step and multiply losses with a linspace\n",
    "            # Because we have more gradients this can result in faster learning\n",
    "            self.anneal_factors = tf.placeholder(tf.float32, sequence_length)\n",
    "            annealed_losses = self._build_annealed_losses(self.ys, self.input_y, self.anneal_factors)\n",
    "            self.loss = tf.reduce_sum(annealed_losses) / batch_size\n",
    "            # self.mean_loss = tf.reduce_mean(annealed_losses)\n",
    "#             elif loss == \"last\":\n",
    "#                 # Standard loss, only last output is considered\n",
    "#                 self.loss = self._build_total_ce_loss(self.ys[-1], self.input_y)\n",
    "#                 self.mean_loss = self._build_mean_ce_loss(self.ys[-1], self.input_y)\n",
    "#             self.loss = self._build_total_ce_loss(self.y, self.input_y) / batch_size\n",
    "#             self.mean_loss = self._build_mean_ce_loss(self.y, self.input_y)                \n",
    "\n",
    "        # Summaries\n",
    "        total_loss_summary = tf.scalar_summary(\"loss\", self.loss)\n",
    "        # mean_loss_summary = tf.scalar_summary(\"mean loss\", self.mean_loss)\n",
    "        accuracy_summmary = tf.scalar_summary(\"accuracy\", self._build_accuracy(self.y, self.input_y))\n",
    "        # self.summaries = tf.merge_all_summaries()\n",
    "\n",
    "    def _build_annealed_losses(self, outputs, labels, anneal_factors):\n",
    "        sequence_length = len(outputs)\n",
    "        packed_outputs = tf.pack(outputs)\n",
    "        tiled_labels = tf.pack([labels for i in range(sequence_length)])\n",
    "        accumulated_losses = -tf.reduce_sum(tiled_labels * tf.log(packed_outputs), [1, 2])\n",
    "        annealed_losses = tf.mul(anneal_factors, tf.concat(0, accumulated_losses))\n",
    "        return annealed_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dennybritz/projects/nlp/sentiment-analysis/playground/runs/1448219033/summaries/train\n",
      "2015-11-22T20:04:55.942345: step 1, loss 0.744782\n",
      "2015-11-22T20:05:02.840205: step 2, loss 0.68902\n",
      "2015-11-22T20:05:08.012602: step 3, loss 0.698698\n",
      "2015-11-22T20:05:13.005930: step 4, loss 0.683569\n",
      "2015-11-22T20:05:18.403183: step 5, loss 0.694814\n",
      "2015-11-22T20:05:23.965348: step 6, loss 0.728951\n",
      "2015-11-22T20:05:30.472632: step 7, loss 0.706998\n",
      "2015-11-22T20:05:36.514957: step 8, loss 0.691523\n",
      "2015-11-22T20:05:42.643903: step 9, loss 0.683378\n",
      "2015-11-22T20:05:48.971263: step 10, loss 0.710833\n",
      "2015-11-22T20:05:54.475252: step 11, loss 0.716153\n",
      "2015-11-22T20:05:59.606446: step 12, loss 0.694478\n",
      "2015-11-22T20:06:06.424328: step 13, loss 0.691105\n",
      "2015-11-22T20:06:11.779212: step 14, loss 0.697388\n",
      "2015-11-22T20:06:17.810242: step 15, loss 0.718421\n",
      "2015-11-22T20:06:25.016755: step 16, loss 0.70462\n",
      "2015-11-22T20:06:30.217342: step 17, loss 0.723766\n",
      "2015-11-22T20:06:36.495360: step 18, loss 0.708701\n",
      "2015-11-22T20:06:41.933395: step 19, loss 0.688276\n",
      "2015-11-22T20:06:47.757780: step 20, loss 0.699756\n",
      "2015-11-22T20:06:53.533713: step 21, loss 0.687338\n",
      "2015-11-22T20:06:58.498652: step 22, loss 0.685597\n",
      "2015-11-22T20:07:03.431796: step 23, loss 0.691307\n",
      "2015-11-22T20:07:08.424287: step 24, loss 0.70474\n",
      "2015-11-22T20:07:13.802526: step 25, loss 0.704077\n",
      "2015-11-22T20:07:18.751578: step 26, loss 0.690978\n",
      "2015-11-22T20:07:24.779079: step 27, loss 0.676683\n",
      "2015-11-22T20:07:29.678296: step 28, loss 0.699101\n",
      "2015-11-22T20:07:35.702170: step 29, loss 0.706823\n",
      "2015-11-22T20:07:41.299283: step 30, loss 0.708971\n",
      "2015-11-22T20:07:46.986492: step 31, loss 0.702982\n",
      "2015-11-22T20:07:51.925592: step 32, loss 0.684544\n",
      "2015-11-22T20:07:57.738253: step 33, loss 0.689243\n",
      "2015-11-22T20:08:03.424287: step 34, loss 0.70558\n",
      "2015-11-22T20:08:08.361051: step 35, loss 0.700637\n",
      "2015-11-22T20:08:15.182684: step 36, loss 0.702076\n",
      "2015-11-22T20:08:22.581473: step 37, loss 0.688259\n",
      "2015-11-22T20:08:30.183280: step 38, loss 0.689342\n",
      "2015-11-22T20:08:36.734283: step 39, loss 0.69196\n",
      "2015-11-22T20:08:42.560865: step 40, loss 0.702723\n",
      "2015-11-22T20:08:49.418483: step 41, loss 0.701267\n",
      "2015-11-22T20:08:56.186658: step 42, loss 0.68421\n",
      "2015-11-22T20:09:03.007497: step 43, loss 0.710301\n",
      "2015-11-22T20:09:09.529609: step 44, loss 0.692596\n",
      "2015-11-22T20:09:15.286430: step 45, loss 0.705558\n",
      "2015-11-22T20:09:21.016106: step 46, loss 0.695506\n",
      "2015-11-22T20:09:29.203753: step 47, loss 0.695118\n",
      "2015-11-22T20:09:35.423628: step 48, loss 0.696741\n",
      "2015-11-22T20:09:41.007692: step 49, loss 0.702081\n",
      "2015-11-22T20:09:47.754597: step 50, loss 0.708622\n",
      "2015-11-22T20:09:54.946018: step 51, loss 0.697397\n",
      "2015-11-22T20:10:01.994936: step 52, loss 0.688464\n",
      "2015-11-22T20:10:08.543310: step 53, loss 0.705193\n",
      "2015-11-22T20:10:15.252494: step 54, loss 0.692288\n",
      "2015-11-22T20:10:21.090509: step 55, loss 0.706535\n",
      "2015-11-22T20:10:28.290754: step 56, loss 0.704687\n",
      "2015-11-22T20:10:34.939786: step 57, loss 0.693756\n",
      "2015-11-22T20:10:40.555435: step 58, loss 0.687143\n",
      "2015-11-22T20:10:45.364849: step 59, loss 0.694772\n",
      "2015-11-22T20:10:50.087076: step 60, loss 0.701628\n",
      "2015-11-22T20:10:54.869777: step 61, loss 0.708912\n",
      "2015-11-22T20:10:59.657492: step 62, loss 0.689852\n",
      "2015-11-22T20:11:04.490818: step 63, loss 0.696424\n",
      "2015-11-22T20:11:09.425524: step 64, loss 0.691754\n",
      "2015-11-22T20:11:14.444992: step 65, loss 0.689432\n",
      "2015-11-22T20:11:19.301941: step 66, loss 0.680007\n",
      "2015-11-22T20:11:24.402157: step 67, loss 0.699764\n",
      "2015-11-22T20:11:29.254991: step 68, loss 0.698689\n",
      "2015-11-22T20:11:34.080105: step 69, loss 0.697509\n",
      "2015-11-22T20:11:39.014544: step 70, loss 0.705728\n",
      "2015-11-22T20:11:43.856094: step 71, loss 0.699394\n",
      "2015-11-22T20:11:48.770322: step 72, loss 0.697628\n",
      "2015-11-22T20:11:53.570313: step 73, loss 0.696034\n",
      "2015-11-22T20:11:58.255082: step 74, loss 0.686071\n",
      "2015-11-22T20:12:03.361807: step 75, loss 0.703913\n",
      "2015-11-22T20:12:08.343544: step 76, loss 0.695838\n",
      "2015-11-22T20:12:13.362023: step 77, loss 0.691686\n",
      "2015-11-22T20:12:18.225913: step 78, loss 0.68843\n",
      "2015-11-22T20:12:23.143505: step 79, loss 0.685319\n",
      "2015-11-22T20:12:27.956807: step 80, loss 0.702524\n",
      "2015-11-22T20:12:32.852781: step 81, loss 0.701873\n",
      "2015-11-22T20:12:37.694277: step 82, loss 0.695647\n",
      "2015-11-22T20:12:42.568523: step 83, loss 0.691399\n",
      "2015-11-22T20:12:47.429702: step 84, loss 0.688085\n",
      "2015-11-22T20:12:52.187048: step 85, loss 0.685375\n",
      "2015-11-22T20:12:57.044923: step 86, loss 0.685901\n",
      "2015-11-22T20:13:01.850799: step 87, loss 0.689943\n",
      "2015-11-22T20:13:06.658584: step 88, loss 0.696023\n",
      "2015-11-22T20:13:11.345600: step 89, loss 0.695526\n",
      "2015-11-22T20:13:16.194844: step 90, loss 0.685487\n",
      "2015-11-22T20:13:21.126504: step 91, loss 0.705738\n",
      "2015-11-22T20:13:25.897237: step 92, loss 0.690111\n",
      "2015-11-22T20:13:30.688871: step 93, loss 0.697757\n",
      "2015-11-22T20:13:35.523775: step 94, loss 0.696436\n",
      "2015-11-22T20:13:40.238772: step 95, loss 0.700978\n",
      "2015-11-22T20:13:44.940745: step 96, loss 0.700307\n",
      "2015-11-22T20:13:49.739388: step 97, loss 0.689064\n",
      "2015-11-22T20:13:54.644649: step 98, loss 0.703214\n",
      "2015-11-22T20:13:59.520233: step 99, loss 0.692582\n",
      "2015-11-22T20:14:04.288267: step 100, loss 0.694835\n",
      "2015-11-22T20:14:09.154772: step 101, loss 0.688384\n",
      "2015-11-22T20:14:14.093512: step 102, loss 0.690973\n",
      "2015-11-22T20:14:18.940480: step 103, loss 0.699506\n",
      "2015-11-22T20:14:23.783400: step 104, loss 0.689727\n",
      "2015-11-22T20:14:28.573944: step 105, loss 0.69383\n",
      "2015-11-22T20:14:33.161337: step 106, loss 0.691514\n",
      "2015-11-22T20:14:38.087376: step 107, loss 0.706373\n",
      "2015-11-22T20:14:42.790341: step 108, loss 0.68879\n",
      "2015-11-22T20:14:47.503409: step 109, loss 0.696092\n",
      "2015-11-22T20:14:52.405368: step 110, loss 0.695174\n",
      "2015-11-22T20:14:57.236265: step 111, loss 0.690313\n",
      "2015-11-22T20:15:02.014834: step 112, loss 0.696786\n",
      "2015-11-22T20:15:06.842115: step 113, loss 0.709831\n",
      "2015-11-22T20:15:11.746470: step 114, loss 0.708987\n",
      "2015-11-22T20:15:16.605911: step 115, loss 0.687213\n",
      "2015-11-22T20:15:21.433839: step 116, loss 0.707511\n",
      "2015-11-22T20:15:26.206704: step 117, loss 0.702504\n",
      "2015-11-22T20:15:30.986893: step 118, loss 0.694533\n",
      "2015-11-22T20:15:35.762810: step 119, loss 0.690853\n",
      "2015-11-22T20:15:40.567693: step 120, loss 0.689103\n",
      "2015-11-22T20:15:45.395338: step 121, loss 0.685599\n",
      "2015-11-22T20:15:50.162645: step 122, loss 0.688914\n",
      "2015-11-22T20:15:55.101467: step 123, loss 0.695008\n",
      "2015-11-22T20:15:59.897989: step 124, loss 0.692925\n",
      "2015-11-22T20:16:04.705018: step 125, loss 0.695589\n",
      "2015-11-22T20:16:09.731771: step 126, loss 0.691541\n",
      "2015-11-22T20:16:14.673707: step 127, loss 0.692066\n",
      "2015-11-22T20:16:19.475727: step 128, loss 0.693407\n",
      "2015-11-22T20:16:24.182483: step 129, loss 0.689393\n",
      "2015-11-22T20:16:28.995956: step 130, loss 0.681864\n",
      "2015-11-22T20:16:33.769754: step 131, loss 0.687964\n",
      "2015-11-22T20:16:38.581048: step 132, loss 0.692618\n",
      "2015-11-22T20:16:43.497976: step 133, loss 0.69915\n",
      "2015-11-22T20:16:48.248057: step 134, loss 0.70589\n",
      "2015-11-22T20:16:53.018382: step 135, loss 0.695317\n",
      "2015-11-22T20:16:57.821570: step 136, loss 0.706231\n",
      "2015-11-22T20:17:02.679964: step 137, loss 0.702668\n",
      "2015-11-22T20:17:07.395467: step 138, loss 0.697649\n",
      "2015-11-22T20:17:12.194024: step 139, loss 0.699095\n",
      "2015-11-22T20:17:16.977183: step 140, loss 0.70161\n",
      "2015-11-22T20:17:21.804827: step 141, loss 0.697889\n",
      "2015-11-22T20:17:26.703367: step 142, loss 0.690397\n",
      "2015-11-22T20:17:31.591220: step 143, loss 0.702052\n",
      "2015-11-22T20:17:36.357674: step 144, loss 0.701692\n",
      "2015-11-22T20:17:41.131216: step 145, loss 0.699141\n",
      "2015-11-22T20:17:45.907669: step 146, loss 0.694514\n",
      "2015-11-22T20:17:50.667347: step 147, loss 0.699864\n",
      "2015-11-22T20:17:55.469397: step 148, loss 0.694982\n",
      "2015-11-22T20:18:00.344018: step 149, loss 0.691496\n",
      "2015-11-22T20:18:05.138267: step 150, loss 0.690937\n",
      "2015-11-22T20:18:09.884091: step 151, loss 0.700224\n",
      "2015-11-22T20:18:14.695136: step 152, loss 0.695412\n",
      "2015-11-22T20:18:19.542234: step 153, loss 0.697627\n",
      "2015-11-22T20:18:24.355720: step 154, loss 0.696949\n",
      "2015-11-22T20:18:29.110492: step 155, loss 0.693861\n",
      "2015-11-22T20:18:33.884271: step 156, loss 0.711835\n",
      "2015-11-22T20:18:38.683957: step 157, loss 0.689886\n",
      "2015-11-22T20:18:43.443022: step 158, loss 0.694589\n",
      "2015-11-22T20:18:48.166113: step 159, loss 0.695544\n",
      "2015-11-22T20:18:52.889585: step 160, loss 0.704157\n",
      "2015-11-22T20:18:57.622015: step 161, loss 0.695887\n",
      "2015-11-22T20:19:02.344586: step 162, loss 0.702435\n",
      "2015-11-22T20:19:07.286671: step 163, loss 0.688361\n",
      "2015-11-22T20:19:12.076932: step 164, loss 0.692806\n",
      "2015-11-22T20:19:16.956606: step 165, loss 0.69072\n",
      "2015-11-22T20:19:21.615765: step 166, loss 0.715712\n",
      "2015-11-22T20:19:26.490305: step 167, loss 0.706669\n",
      "2015-11-22T20:19:31.292796: step 168, loss 0.68852\n",
      "2015-11-22T20:19:36.150968: step 169, loss 0.702472\n",
      "2015-11-22T20:19:40.969782: step 170, loss 0.696409\n",
      "2015-11-22T20:19:45.778895: step 171, loss 0.681229\n",
      "2015-11-22T20:19:50.609156: step 172, loss 0.692416\n",
      "2015-11-22T20:19:55.457027: step 173, loss 0.686764\n",
      "2015-11-22T20:20:00.239883: step 174, loss 0.692052\n",
      "2015-11-22T20:20:05.014804: step 175, loss 0.701216\n",
      "2015-11-22T20:20:09.749905: step 176, loss 0.703174\n",
      "2015-11-22T20:20:14.539435: step 177, loss 0.703563\n",
      "2015-11-22T20:20:19.379098: step 178, loss 0.69012\n",
      "2015-11-22T20:20:24.105780: step 179, loss 0.695117\n",
      "2015-11-22T20:20:28.809261: step 180, loss 0.689606\n",
      "2015-11-22T20:20:33.601692: step 181, loss 0.697619\n",
      "2015-11-22T20:20:38.465537: step 182, loss 0.686318\n",
      "2015-11-22T20:20:43.235715: step 183, loss 0.704662\n",
      "2015-11-22T20:20:47.946601: step 184, loss 0.688523\n",
      "2015-11-22T20:20:52.818145: step 185, loss 0.694835\n",
      "2015-11-22T20:20:57.668712: step 186, loss 0.699153\n",
      "2015-11-22T20:21:02.498408: step 187, loss 0.684902\n",
      "2015-11-22T20:21:07.328454: step 188, loss 0.68381\n",
      "2015-11-22T20:21:12.057401: step 189, loss 0.703627\n",
      "2015-11-22T20:21:16.876435: step 190, loss 0.69356\n",
      "2015-11-22T20:21:21.712215: step 191, loss 0.694674\n",
      "2015-11-22T20:21:26.458527: step 192, loss 0.68906\n",
      "2015-11-22T20:21:31.172509: step 193, loss 0.69686\n",
      "2015-11-22T20:21:35.969929: step 194, loss 0.690929\n",
      "2015-11-22T20:21:40.734627: step 195, loss 0.705811\n",
      "2015-11-22T20:21:45.579874: step 196, loss 0.694482\n",
      "2015-11-22T20:21:50.308563: step 197, loss 0.692372\n",
      "2015-11-22T20:21:55.137606: step 198, loss 0.696476\n",
      "2015-11-22T20:21:59.964662: step 199, loss 0.696681\n",
      "2015-11-22T20:22:04.847775: step 200, loss 0.692941\n",
      "2015-11-22T20:22:09.604963: step 201, loss 0.6911\n",
      "2015-11-22T20:22:14.450245: step 202, loss 0.702078\n",
      "2015-11-22T20:22:19.206639: step 203, loss 0.699959\n",
      "2015-11-22T20:22:23.953580: step 204, loss 0.697124\n",
      "2015-11-22T20:22:28.747839: step 205, loss 0.694896\n",
      "2015-11-22T20:22:33.468911: step 206, loss 0.695766\n",
      "2015-11-22T20:22:38.256424: step 207, loss 0.691115\n",
      "2015-11-22T20:22:43.069014: step 208, loss 0.693859\n",
      "2015-11-22T20:22:47.845135: step 209, loss 0.692658\n",
      "2015-11-22T20:22:52.558176: step 210, loss 0.695331\n",
      "2015-11-22T20:22:57.356318: step 211, loss 0.701508\n",
      "2015-11-22T20:23:02.204381: step 212, loss 0.695043\n",
      "2015-11-22T20:23:06.856187: step 213, loss 0.696933\n",
      "2015-11-22T20:23:11.628430: step 214, loss 0.697554\n",
      "2015-11-22T20:23:16.296625: step 215, loss 0.696465\n",
      "2015-11-22T20:23:21.020419: step 216, loss 0.691088\n",
      "2015-11-22T20:23:25.787353: step 217, loss 0.700502\n",
      "2015-11-22T20:23:30.477950: step 218, loss 0.697921\n",
      "2015-11-22T20:23:35.325452: step 219, loss 0.693536\n",
      "2015-11-22T20:23:40.107760: step 220, loss 0.696377\n",
      "2015-11-22T20:23:44.857171: step 221, loss 0.696356\n",
      "2015-11-22T20:23:49.641939: step 222, loss 0.694188\n",
      "2015-11-22T20:23:54.438591: step 223, loss 0.695041\n",
      "2015-11-22T20:23:59.210990: step 224, loss 0.696739\n",
      "2015-11-22T20:24:04.047765: step 225, loss 0.698602\n",
      "2015-11-22T20:24:08.923003: step 226, loss 0.695463\n",
      "2015-11-22T20:24:13.705456: step 227, loss 0.690199\n",
      "2015-11-22T20:24:18.393821: step 228, loss 0.693518\n",
      "2015-11-22T20:24:23.172948: step 229, loss 0.701804\n",
      "2015-11-22T20:24:27.962406: step 230, loss 0.690276\n",
      "2015-11-22T20:24:32.715548: step 231, loss 0.69831\n",
      "2015-11-22T20:24:37.544793: step 232, loss 0.698117\n",
      "2015-11-22T20:24:42.263146: step 233, loss 0.690479\n",
      "2015-11-22T20:24:46.963956: step 234, loss 0.690831\n",
      "2015-11-22T20:24:51.711010: step 235, loss 0.698626\n",
      "2015-11-22T20:24:56.581521: step 236, loss 0.687647\n",
      "2015-11-22T20:25:01.369350: step 237, loss 0.696888\n",
      "2015-11-22T20:25:06.160798: step 238, loss 0.701345\n",
      "2015-11-22T20:25:11.046116: step 239, loss 0.69843\n",
      "2015-11-22T20:25:15.978665: step 240, loss 0.689387\n",
      "2015-11-22T20:25:20.764145: step 241, loss 0.698032\n",
      "2015-11-22T20:25:25.613927: step 242, loss 0.691764\n",
      "2015-11-22T20:25:30.460151: step 243, loss 0.689673\n",
      "2015-11-22T20:25:35.209976: step 244, loss 0.696191\n",
      "2015-11-22T20:25:40.014606: step 245, loss 0.683753\n",
      "2015-11-22T20:25:44.843088: step 246, loss 0.692617\n",
      "2015-11-22T20:25:49.726399: step 247, loss 0.702413\n",
      "2015-11-22T20:25:54.600671: step 248, loss 0.6956\n",
      "2015-11-22T20:25:59.323940: step 249, loss 0.679089\n",
      "2015-11-22T20:26:04.224105: step 250, loss 0.686936\n",
      "2015-11-22T20:26:09.040562: step 251, loss 0.684623\n",
      "2015-11-22T20:26:13.698436: step 252, loss 0.698385\n",
      "2015-11-22T20:26:18.409419: step 253, loss 0.702168\n",
      "2015-11-22T20:26:23.278348: step 254, loss 0.695484\n",
      "2015-11-22T20:26:28.017656: step 255, loss 0.70689\n",
      "2015-11-22T20:26:32.862540: step 256, loss 0.721362\n",
      "2015-11-22T20:26:37.753890: step 257, loss 0.701172\n",
      "2015-11-22T20:26:42.459694: step 258, loss 0.69897\n",
      "2015-11-22T20:26:47.367851: step 259, loss 0.696341\n",
      "2015-11-22T20:26:52.177855: step 260, loss 0.692768\n",
      "2015-11-22T20:26:57.067989: step 261, loss 0.683309\n",
      "2015-11-22T20:27:01.772758: step 262, loss 0.679498\n",
      "2015-11-22T20:27:06.600986: step 263, loss 0.683533\n",
      "2015-11-22T20:27:11.466892: step 264, loss 0.682034\n",
      "2015-11-22T20:27:16.288431: step 265, loss 0.704943\n",
      "2015-11-22T20:27:21.019060: step 266, loss 0.685853\n",
      "2015-11-22T20:27:25.810168: step 267, loss 0.715214\n",
      "2015-11-22T20:27:30.661166: step 268, loss 0.701887\n",
      "2015-11-22T20:27:35.485316: step 269, loss 0.716612\n",
      "2015-11-22T20:27:40.299758: step 270, loss 0.703206\n",
      "2015-11-22T20:27:45.063916: step 271, loss 0.689388\n",
      "2015-11-22T20:27:49.911420: step 272, loss 0.681396\n",
      "2015-11-22T20:27:54.764785: step 273, loss 0.680004\n",
      "2015-11-22T20:27:59.469698: step 274, loss 0.682917\n",
      "2015-11-22T20:28:04.213895: step 275, loss 0.695651\n",
      "2015-11-22T20:28:09.012702: step 276, loss 0.689628\n",
      "2015-11-22T20:28:13.817766: step 277, loss 0.690296\n",
      "2015-11-22T20:28:18.631621: step 278, loss 0.695463\n",
      "2015-11-22T20:28:23.562369: step 279, loss 0.695032\n",
      "2015-11-22T20:28:28.406240: step 280, loss 0.69634\n",
      "2015-11-22T20:28:33.219193: step 281, loss 0.68819\n",
      "2015-11-22T20:28:38.092715: step 282, loss 0.690353\n",
      "2015-11-22T20:28:42.944379: step 283, loss 0.69776\n",
      "2015-11-22T20:28:47.658697: step 284, loss 0.680871\n",
      "2015-11-22T20:28:52.441904: step 285, loss 0.6878\n",
      "2015-11-22T20:28:57.220913: step 286, loss 0.703282\n",
      "2015-11-22T20:29:01.995680: step 287, loss 0.705442\n",
      "2015-11-22T20:29:06.735891: step 288, loss 0.694763\n",
      "2015-11-22T20:29:11.451329: step 289, loss 0.696555\n",
      "2015-11-22T20:29:16.238632: step 290, loss 0.693376\n",
      "2015-11-22T20:29:20.987480: step 291, loss 0.69511\n",
      "2015-11-22T20:29:25.820744: step 292, loss 0.699998\n",
      "2015-11-22T20:29:30.564439: step 293, loss 0.699129\n",
      "2015-11-22T20:29:35.428925: step 294, loss 0.695827\n",
      "2015-11-22T20:29:40.357137: step 295, loss 0.702322\n",
      "2015-11-22T20:29:45.217995: step 296, loss 0.693017\n",
      "2015-11-22T20:29:50.015043: step 297, loss 0.692408\n",
      "2015-11-22T20:29:54.858767: step 298, loss 0.702609\n",
      "2015-11-22T20:29:59.668225: step 299, loss 0.710552\n",
      "2015-11-22T20:30:04.478979: step 300, loss 0.694975\n",
      "2015-11-22T20:30:09.291748: step 301, loss 0.695212\n",
      "2015-11-22T20:30:14.121443: step 302, loss 0.694719\n",
      "2015-11-22T20:30:19.002173: step 303, loss 0.699588\n",
      "2015-11-22T20:30:23.843220: step 304, loss 0.701638\n",
      "2015-11-22T20:30:28.577510: step 305, loss 0.692486\n",
      "2015-11-22T20:30:33.346796: step 306, loss 0.692586\n",
      "2015-11-22T20:30:38.085474: step 307, loss 0.688124\n",
      "2015-11-22T20:30:42.882529: step 308, loss 0.696052\n",
      "2015-11-22T20:30:47.651626: step 309, loss 0.701378\n",
      "2015-11-22T20:30:52.482048: step 310, loss 0.690123\n",
      "2015-11-22T20:30:57.124189: step 311, loss 0.694935\n",
      "2015-11-22T20:31:01.968799: step 312, loss 0.695689\n",
      "2015-11-22T20:31:06.751656: step 313, loss 0.697502\n",
      "2015-11-22T20:31:11.668564: step 314, loss 0.700693\n",
      "2015-11-22T20:31:16.515335: step 315, loss 0.697603\n",
      "2015-11-22T20:31:21.300125: step 316, loss 0.707272\n",
      "2015-11-22T20:31:26.122112: step 317, loss 0.685585\n",
      "2015-11-22T20:31:30.973024: step 318, loss 0.694165\n",
      "2015-11-22T20:31:36.841902: step 319, loss 0.696926\n",
      "2015-11-22T20:31:43.926065: step 320, loss 0.696166\n",
      "2015-11-22T20:31:51.255906: step 321, loss 0.699052\n",
      "2015-11-22T20:31:58.986513: step 322, loss 0.687592\n",
      "2015-11-22T20:32:06.220025: step 323, loss 0.689182\n",
      "2015-11-22T20:32:13.693667: step 324, loss 0.697819\n",
      "2015-11-22T20:32:19.872583: step 325, loss 0.686146\n",
      "2015-11-22T20:32:25.871561: step 326, loss 0.681544\n",
      "2015-11-22T20:32:32.784656: step 327, loss 0.703536\n",
      "2015-11-22T20:32:38.970259: step 328, loss 0.701058\n",
      "2015-11-22T20:32:45.863258: step 329, loss 0.688392\n",
      "2015-11-22T20:32:53.550733: step 330, loss 0.688443\n",
      "2015-11-22T20:33:00.021140: step 331, loss 0.69193\n",
      "2015-11-22T20:33:05.823999: step 332, loss 0.686806\n",
      "2015-11-22T20:33:10.618252: step 333, loss 0.705845\n",
      "2015-11-22T20:33:15.310175: step 334, loss 0.695548\n",
      "2015-11-22T20:33:20.104781: step 335, loss 0.69741\n",
      "2015-11-22T20:33:24.775707: step 336, loss 0.687282\n",
      "2015-11-22T20:33:29.437422: step 337, loss 0.694561\n",
      "2015-11-22T20:33:34.091968: step 338, loss 0.695526\n",
      "2015-11-22T20:33:38.805614: step 339, loss 0.691055\n",
      "2015-11-22T20:33:43.462198: step 340, loss 0.703877\n",
      "2015-11-22T20:33:48.116977: step 341, loss 0.685445\n",
      "2015-11-22T20:33:52.854266: step 342, loss 0.701112\n",
      "2015-11-22T20:33:57.554595: step 343, loss 0.692652\n",
      "2015-11-22T20:34:02.241400: step 344, loss 0.70016\n",
      "2015-11-22T20:34:06.984763: step 345, loss 0.69394\n",
      "2015-11-22T20:34:11.769450: step 346, loss 0.692121\n",
      "2015-11-22T20:34:16.514606: step 347, loss 0.691481\n",
      "2015-11-22T20:34:21.276891: step 348, loss 0.68279\n",
      "2015-11-22T20:34:26.032935: step 349, loss 0.686278\n",
      "2015-11-22T20:34:30.802806: step 350, loss 0.703058\n",
      "2015-11-22T20:34:35.467296: step 351, loss 0.696173\n",
      "2015-11-22T20:34:40.211884: step 352, loss 0.695407\n",
      "2015-11-22T20:34:44.836613: step 353, loss 0.690444\n",
      "2015-11-22T20:34:49.576384: step 354, loss 0.696235\n",
      "2015-11-22T20:34:54.318371: step 355, loss 0.694878\n",
      "2015-11-22T20:34:58.911724: step 356, loss 0.696436\n",
      "2015-11-22T20:35:03.589991: step 357, loss 0.69891\n",
      "2015-11-22T20:35:08.339806: step 358, loss 0.693243\n",
      "2015-11-22T20:35:13.105749: step 359, loss 0.691968\n",
      "2015-11-22T20:35:17.751834: step 360, loss 0.702455\n",
      "2015-11-22T20:35:22.482122: step 361, loss 0.693575\n",
      "2015-11-22T20:35:27.255357: step 362, loss 0.697959\n",
      "2015-11-22T20:35:31.960079: step 363, loss 0.704861\n",
      "2015-11-22T20:35:36.640621: step 364, loss 0.703949\n",
      "2015-11-22T20:35:41.449193: step 365, loss 0.693472\n",
      "2015-11-22T20:35:46.205582: step 366, loss 0.690154\n",
      "2015-11-22T20:35:50.949915: step 367, loss 0.69233\n",
      "2015-11-22T20:35:55.706377: step 368, loss 0.70008\n",
      "2015-11-22T20:36:00.429687: step 369, loss 0.69776\n",
      "2015-11-22T20:36:05.138691: step 370, loss 0.692298\n",
      "2015-11-22T20:36:09.824072: step 371, loss 0.700918\n",
      "2015-11-22T20:36:14.487770: step 372, loss 0.698898\n",
      "2015-11-22T20:36:19.123274: step 373, loss 0.69644\n",
      "2015-11-22T20:36:23.837609: step 374, loss 0.701205\n",
      "2015-11-22T20:36:28.484983: step 375, loss 0.704695\n",
      "2015-11-22T20:36:33.121581: step 376, loss 0.702649\n",
      "2015-11-22T20:36:37.798881: step 377, loss 0.69535\n",
      "2015-11-22T20:36:42.466286: step 378, loss 0.702002\n",
      "2015-11-22T20:36:47.118804: step 379, loss 0.701067\n",
      "2015-11-22T20:36:51.743336: step 380, loss 0.697117\n",
      "2015-11-22T20:36:56.378903: step 381, loss 0.693065\n",
      "2015-11-22T20:37:01.063660: step 382, loss 0.706399\n",
      "2015-11-22T20:37:05.759316: step 383, loss 0.697373\n",
      "2015-11-22T20:37:10.435168: step 384, loss 0.704145\n",
      "2015-11-22T20:37:15.209021: step 385, loss 0.690599\n",
      "2015-11-22T20:37:19.888994: step 386, loss 0.697943\n",
      "2015-11-22T20:37:24.669323: step 387, loss 0.703902\n",
      "2015-11-22T20:37:29.351665: step 388, loss 0.687082\n",
      "2015-11-22T20:37:34.097013: step 389, loss 0.696028\n",
      "2015-11-22T20:37:38.756745: step 390, loss 0.690785\n",
      "2015-11-22T20:37:43.490108: step 391, loss 0.701899\n",
      "2015-11-22T20:37:48.059234: step 392, loss 0.692262\n",
      "2015-11-22T20:37:52.695140: step 393, loss 0.692707\n",
      "2015-11-22T20:37:57.503243: step 394, loss 0.693776\n",
      "2015-11-22T20:38:02.366407: step 395, loss 0.683832\n",
      "2015-11-22T20:38:07.091883: step 396, loss 0.698388\n",
      "2015-11-22T20:38:11.721839: step 397, loss 0.69368\n",
      "2015-11-22T20:38:16.532723: step 398, loss 0.693681\n",
      "2015-11-22T20:38:21.237566: step 399, loss 0.689413\n",
      "2015-11-22T20:38:25.932036: step 400, loss 0.692185\n",
      "2015-11-22T20:38:30.622685: step 401, loss 0.696129\n",
      "2015-11-22T20:38:35.564139: step 402, loss 0.698181\n",
      "2015-11-22T20:38:40.388075: step 403, loss 0.694782\n",
      "2015-11-22T20:38:45.077107: step 404, loss 0.715238\n",
      "2015-11-22T20:38:49.764124: step 405, loss 0.709228\n",
      "2015-11-22T20:38:54.546568: step 406, loss 0.688119\n",
      "2015-11-22T20:38:59.310442: step 407, loss 0.703676\n",
      "2015-11-22T20:39:04.027152: step 408, loss 0.700868\n",
      "2015-11-22T20:39:08.769962: step 409, loss 0.690785\n",
      "2015-11-22T20:39:13.447196: step 410, loss 0.692518\n",
      "2015-11-22T20:39:18.243507: step 411, loss 0.687889\n",
      "2015-11-22T20:39:22.805637: step 412, loss 0.689885\n",
      "2015-11-22T20:39:27.406663: step 413, loss 0.698712\n",
      "2015-11-22T20:39:32.165030: step 414, loss 0.709583\n",
      "2015-11-22T20:39:36.775210: step 415, loss 0.702881\n",
      "2015-11-22T20:39:41.445927: step 416, loss 0.680061\n",
      "2015-11-22T20:39:46.071418: step 417, loss 0.707592\n",
      "2015-11-22T20:39:50.736833: step 418, loss 0.699129\n",
      "2015-11-22T20:39:55.608452: step 419, loss 0.690346\n",
      "2015-11-22T20:40:00.272129: step 420, loss 0.695714\n",
      "2015-11-22T20:40:05.043906: step 421, loss 0.697859\n",
      "2015-11-22T20:40:09.855850: step 422, loss 0.69391\n",
      "2015-11-22T20:40:14.419838: step 423, loss 0.694966\n",
      "2015-11-22T20:40:19.048551: step 424, loss 0.69229\n",
      "2015-11-22T20:40:23.757348: step 425, loss 0.703752\n",
      "2015-11-22T20:40:28.468097: step 426, loss 0.694757\n",
      "2015-11-22T20:40:33.072709: step 427, loss 0.689394\n",
      "2015-11-22T20:40:37.704151: step 428, loss 0.698961\n",
      "2015-11-22T20:40:42.455885: step 429, loss 0.698425\n",
      "2015-11-22T20:40:47.119114: step 430, loss 0.694817\n",
      "2015-11-22T20:40:51.706665: step 431, loss 0.69433\n",
      "2015-11-22T20:40:56.390272: step 432, loss 0.6968\n",
      "2015-11-22T20:41:01.297815: step 433, loss 0.686496\n",
      "2015-11-22T20:41:06.018731: step 434, loss 0.690064\n",
      "2015-11-22T20:41:10.736119: step 435, loss 0.687886\n",
      "2015-11-22T20:41:15.580458: step 436, loss 0.693078\n",
      "2015-11-22T20:41:20.358679: step 437, loss 0.688304\n",
      "2015-11-22T20:41:25.182947: step 438, loss 0.693515\n",
      "2015-11-22T20:41:29.925157: step 439, loss 0.692639\n",
      "2015-11-22T20:41:34.651714: step 440, loss 0.694533\n",
      "2015-11-22T20:41:39.397383: step 441, loss 0.694737\n",
      "2015-11-22T20:41:44.081299: step 442, loss 0.692419\n",
      "2015-11-22T20:41:48.797292: step 443, loss 0.697352\n",
      "2015-11-22T20:41:53.562382: step 444, loss 0.690231\n",
      "2015-11-22T20:41:58.215857: step 445, loss 0.700068\n",
      "2015-11-22T20:42:02.882879: step 446, loss 0.703966\n",
      "2015-11-22T20:42:07.752024: step 447, loss 0.694739\n",
      "2015-11-22T20:42:12.399747: step 448, loss 0.691506\n",
      "2015-11-22T20:42:17.044054: step 449, loss 0.691978\n",
      "2015-11-22T20:42:21.758648: step 450, loss 0.703367\n",
      "2015-11-22T20:42:26.391741: step 451, loss 0.691301\n",
      "2015-11-22T20:42:31.111116: step 452, loss 0.700304\n",
      "2015-11-22T20:42:35.818951: step 453, loss 0.694214\n",
      "2015-11-22T20:42:40.497823: step 454, loss 0.694607\n",
      "2015-11-22T20:42:45.159237: step 455, loss 0.696448\n",
      "2015-11-22T20:42:49.878735: step 456, loss 0.693368\n",
      "2015-11-22T20:42:54.492489: step 457, loss 0.692461\n",
      "2015-11-22T20:42:59.039960: step 458, loss 0.699171\n",
      "2015-11-22T20:43:04.073105: step 459, loss 0.692291\n",
      "2015-11-22T20:43:09.003022: step 460, loss 0.690335\n",
      "2015-11-22T20:43:13.652126: step 461, loss 0.68859\n",
      "2015-11-22T20:43:18.402054: step 462, loss 0.697797\n",
      "2015-11-22T20:54:55.218456: step 463, loss 0.694998\n",
      "2015-11-22T20:55:01.177750: step 464, loss 0.693495\n",
      "2015-11-22T20:55:09.744757: step 465, loss 0.697253\n",
      "2015-11-22T20:55:15.021502: step 466, loss 0.695738\n",
      "2015-11-22T20:55:19.950088: step 467, loss 0.70311\n",
      "2015-11-22T20:55:25.035863: step 468, loss 0.699756\n",
      "2015-11-22T20:55:29.894497: step 469, loss 0.692147\n",
      "2015-11-22T20:55:34.710021: step 470, loss 0.691372\n",
      "2015-11-22T20:55:39.531704: step 471, loss 0.699488\n",
      "2015-11-22T20:55:44.540920: step 472, loss 0.701038\n",
      "2015-11-22T20:55:49.421938: step 473, loss 0.691714\n",
      "2015-11-22T20:55:56.422533: step 474, loss 0.69238\n",
      "2015-11-22T20:56:01.258469: step 475, loss 0.693438\n",
      "2015-11-22T20:56:05.924968: step 476, loss 0.696301\n",
      "2015-11-22T20:56:10.751306: step 477, loss 0.695335\n",
      "2015-11-22T20:56:15.747253: step 478, loss 0.690001\n",
      "2015-11-22T20:56:22.512943: step 479, loss 0.698763\n",
      "2015-11-22T20:56:29.846229: step 480, loss 0.68869\n",
      "2015-11-22T20:56:36.213703: step 481, loss 0.692768\n",
      "2015-11-22T20:56:42.440156: step 482, loss 0.692659\n",
      "2015-11-22T20:56:48.753254: step 483, loss 0.686014\n",
      "2015-11-22T20:56:54.886199: step 484, loss 0.694239\n",
      "2015-11-22T20:57:01.471203: step 485, loss 0.709635\n",
      "2015-11-22T20:57:07.778895: step 486, loss 0.696182\n",
      "2015-11-22T20:57:13.860930: step 487, loss 0.70311\n",
      "2015-11-22T20:57:21.580188: step 488, loss 0.690987\n",
      "2015-11-22T20:57:31.399732: step 489, loss 0.697886\n",
      "2015-11-22T20:57:40.350272: step 490, loss 0.682373\n",
      "2015-11-22T20:57:49.713113: step 491, loss 0.701943\n",
      "2015-11-22T20:57:57.067665: step 492, loss 0.690388\n",
      "2015-11-22T20:58:03.563964: step 493, loss 0.69876\n",
      "2015-11-22T20:58:09.446072: step 494, loss 0.68619\n",
      "2015-11-22T20:58:15.305073: step 495, loss 0.693214\n",
      "2015-11-22T20:58:21.742987: step 496, loss 0.693907\n",
      "2015-11-22T20:58:27.332365: step 497, loss 0.688002\n",
      "2015-11-22T20:58:33.267624: step 498, loss 0.690058\n",
      "2015-11-22T20:58:39.819971: step 499, loss 0.700711\n",
      "2015-11-22T20:58:46.748449: step 500, loss 0.703816\n",
      "2015-11-22T20:58:52.616805: step 501, loss 0.715868\n",
      "2015-11-22T20:58:58.721716: step 502, loss 0.692464\n",
      "2015-11-22T20:59:04.854398: step 503, loss 0.714029\n",
      "2015-11-22T20:59:10.594320: step 504, loss 0.697447\n",
      "2015-11-22T20:59:16.297129: step 505, loss 0.694538\n",
      "2015-11-22T20:59:22.053728: step 506, loss 0.695019\n",
      "2015-11-22T20:59:28.604139: step 507, loss 0.692697\n",
      "2015-11-22T20:59:34.354578: step 508, loss 0.686425\n",
      "2015-11-22T20:59:40.017703: step 509, loss 0.698063\n",
      "2015-11-22T20:59:45.793793: step 510, loss 0.698596\n",
      "2015-11-22T20:59:51.630940: step 511, loss 0.698145\n",
      "2015-11-22T20:59:57.354232: step 512, loss 0.689137\n",
      "2015-11-22T21:00:03.102684: step 513, loss 0.692984\n",
      "2015-11-22T21:00:08.860663: step 514, loss 0.695844\n",
      "2015-11-22T21:00:14.653799: step 515, loss 0.702012\n",
      "2015-11-22T21:00:20.469561: step 516, loss 0.706864\n",
      "2015-11-22T21:00:27.266292: step 517, loss 0.705414\n",
      "2015-11-22T21:00:33.662751: step 518, loss 0.694843\n",
      "2015-11-22T21:00:39.360957: step 519, loss 0.709962\n",
      "2015-11-22T21:00:45.071527: step 520, loss 0.691329\n",
      "2015-11-22T21:00:51.370991: step 521, loss 0.700908\n",
      "2015-11-22T21:00:57.990381: step 522, loss 0.694308\n",
      "2015-11-22T21:01:04.909748: step 523, loss 0.697996\n",
      "2015-11-22T21:01:11.450004: step 524, loss 0.706204\n",
      "2015-11-22T21:01:17.306663: step 525, loss 0.694875\n",
      "2015-11-22T21:01:23.083669: step 526, loss 0.714527\n",
      "2015-11-22T21:01:29.286965: step 527, loss 0.712862\n",
      "2015-11-22T21:01:36.492339: step 528, loss 0.701453\n",
      "2015-11-22T21:01:44.040008: step 529, loss 0.690721\n",
      "2015-11-22T21:01:50.200166: step 530, loss 0.690581\n",
      "2015-11-22T21:01:55.915768: step 531, loss 0.697659\n",
      "2015-11-22T21:02:02.357948: step 532, loss 0.689471\n",
      "2015-11-22T21:02:08.133819: step 533, loss 0.699449\n",
      "2015-11-22T21:02:14.539622: step 534, loss 0.6954\n",
      "2015-11-22T21:02:20.838229: step 535, loss 0.697279\n",
      "2015-11-22T21:02:27.424887: step 536, loss 0.686969\n",
      "2015-11-22T21:02:33.332172: step 537, loss 0.695536\n",
      "2015-11-22T21:02:40.192434: step 538, loss 0.69405\n",
      "2015-11-22T21:02:46.561963: step 539, loss 0.690545\n",
      "2015-11-22T21:02:52.549412: step 540, loss 0.697596\n",
      "2015-11-22T21:02:58.664841: step 541, loss 0.691221\n",
      "2015-11-22T21:03:04.744400: step 542, loss 0.691027\n",
      "2015-11-22T21:03:10.438335: step 543, loss 0.696814\n",
      "2015-11-22T21:03:16.646154: step 544, loss 0.690681\n",
      "2015-11-22T21:03:23.665240: step 545, loss 0.690322\n",
      "2015-11-22T21:03:29.887485: step 546, loss 0.69257\n",
      "2015-11-22T21:03:36.175658: step 547, loss 0.692865\n",
      "2015-11-22T21:03:42.289569: step 548, loss 0.692702\n",
      "2015-11-22T21:03:48.387611: step 549, loss 0.696587\n",
      "2015-11-22T21:03:54.226988: step 550, loss 0.694571\n",
      "2015-11-22T21:04:00.292047: step 551, loss 0.688966\n",
      "2015-11-22T21:04:06.842642: step 552, loss 0.699105\n",
      "2015-11-22T21:04:12.729266: step 553, loss 0.702918\n",
      "2015-11-22T21:04:18.633220: step 554, loss 0.687556\n",
      "2015-11-22T21:04:24.522294: step 555, loss 0.700971\n",
      "2015-11-22T21:04:31.104076: step 556, loss 0.703243\n",
      "2015-11-22T21:04:36.777847: step 557, loss 0.695124\n",
      "2015-11-22T21:04:42.644816: step 558, loss 0.694958\n",
      "2015-11-22T21:04:48.500365: step 559, loss 0.712067\n",
      "2015-11-22T21:04:55.123469: step 560, loss 0.685844\n",
      "2015-11-22T21:05:00.962788: step 561, loss 0.694596\n",
      "2015-11-22T21:05:06.896480: step 562, loss 0.692481\n",
      "2015-11-22T21:05:12.749297: step 563, loss 0.696257\n",
      "2015-11-22T21:05:19.440467: step 564, loss 0.694806\n",
      "2015-11-22T21:05:25.313073: step 565, loss 0.700336\n",
      "2015-11-22T21:05:31.086102: step 566, loss 0.69444\n",
      "2015-11-22T21:05:36.807231: step 567, loss 0.696587\n",
      "2015-11-22T21:05:42.705423: step 568, loss 0.69337\n",
      "2015-11-22T21:05:48.559221: step 569, loss 0.692083\n",
      "2015-11-22T21:05:54.363113: step 570, loss 0.698065\n",
      "2015-11-22T21:06:00.779040: step 571, loss 0.696751\n",
      "2015-11-22T21:06:06.687646: step 572, loss 0.689837\n",
      "2015-11-22T21:06:12.493368: step 573, loss 0.689607\n",
      "2015-11-22T21:06:18.472502: step 574, loss 0.694376\n",
      "2015-11-22T21:06:24.693841: step 575, loss 0.696556\n",
      "2015-11-22T21:06:31.590602: step 576, loss 0.69284\n",
      "2015-11-22T21:06:37.867889: step 577, loss 0.701135\n",
      "2015-11-22T21:06:44.735923: step 578, loss 0.699001\n",
      "2015-11-22T21:06:51.371000: step 579, loss 0.695312\n",
      "2015-11-22T21:06:57.826479: step 580, loss 0.692244\n",
      "2015-11-22T21:07:04.719086: step 581, loss 0.695043\n",
      "2015-11-22T21:07:10.525168: step 582, loss 0.69404\n",
      "2015-11-22T21:07:16.430112: step 583, loss 0.691911\n",
      "2015-11-22T21:07:22.229356: step 584, loss 0.689673\n",
      "2015-11-22T21:07:28.082439: step 585, loss 0.696606\n",
      "2015-11-22T21:07:35.214110: step 586, loss 0.700583\n",
      "2015-11-22T21:07:41.936915: step 587, loss 0.700513\n",
      "2015-11-22T21:07:47.713164: step 588, loss 0.69544\n",
      "2015-11-22T21:07:54.122053: step 589, loss 0.694124\n",
      "2015-11-22T21:07:59.995501: step 590, loss 0.69806\n",
      "2015-11-22T21:08:05.958427: step 591, loss 0.693432\n",
      "2015-11-22T21:08:11.770193: step 592, loss 0.701213\n",
      "2015-11-22T21:08:17.948600: step 593, loss 0.702376\n",
      "2015-11-22T21:08:24.543126: step 594, loss 0.694902\n",
      "2015-11-22T21:08:30.333407: step 595, loss 0.704163\n",
      "2015-11-22T21:08:36.787297: step 596, loss 0.694161\n",
      "2015-11-22T21:08:42.570582: step 597, loss 0.695802\n",
      "2015-11-22T21:08:48.477365: step 598, loss 0.70025\n",
      "2015-11-22T21:08:54.466405: step 599, loss 0.697761\n",
      "2015-11-22T21:09:00.329932: step 600, loss 0.695234\n",
      "2015-11-22T21:09:06.088913: step 601, loss 0.693691\n",
      "2015-11-22T21:09:12.270285: step 602, loss 0.694715\n",
      "2015-11-22T21:09:18.578900: step 603, loss 0.704184\n",
      "2015-11-22T21:09:25.041026: step 604, loss 0.693016\n",
      "2015-11-22T21:09:30.753070: step 605, loss 0.693448\n",
      "2015-11-22T21:09:36.562411: step 606, loss 0.699167\n",
      "2015-11-22T21:09:42.423490: step 607, loss 0.688394\n",
      "2015-11-22T21:09:48.319257: step 608, loss 0.701709\n",
      "2015-11-22T21:09:54.040960: step 609, loss 0.696868\n",
      "2015-11-22T21:09:59.953232: step 610, loss 0.697879\n",
      "2015-11-22T21:10:06.497848: step 611, loss 0.691488\n",
      "2015-11-22T21:10:13.759523: step 612, loss 0.692975\n",
      "2015-11-22T21:10:20.521943: step 613, loss 0.688353\n",
      "2015-11-22T21:10:27.387959: step 614, loss 0.699843\n",
      "2015-11-22T21:10:33.251712: step 615, loss 0.698975\n",
      "2015-11-22T21:10:39.041587: step 616, loss 0.700669\n",
      "2015-11-22T21:10:45.709243: step 617, loss 0.694739\n",
      "2015-11-22T21:10:52.647207: step 618, loss 0.706877\n",
      "2015-11-22T21:10:58.967376: step 619, loss 0.690447\n",
      "2015-11-22T21:11:04.733216: step 620, loss 0.695809\n",
      "2015-11-22T21:11:10.552576: step 621, loss 0.695556\n",
      "2015-11-22T21:11:16.414366: step 622, loss 0.693813\n",
      "2015-11-22T21:11:22.185753: step 623, loss 0.699383\n",
      "2015-11-22T21:11:28.053815: step 624, loss 0.696982\n",
      "2015-11-22T21:11:33.840591: step 625, loss 0.697089\n",
      "2015-11-22T21:11:39.556411: step 626, loss 0.693623\n",
      "2015-11-22T21:11:45.202555: step 627, loss 0.693231\n",
      "2015-11-22T21:11:50.833457: step 628, loss 0.696376\n",
      "2015-11-22T21:11:56.555043: step 629, loss 0.69445\n",
      "2015-11-22T21:12:02.401250: step 630, loss 0.704153\n",
      "2015-11-22T21:12:08.153564: step 631, loss 0.702053\n",
      "2015-11-22T21:12:14.205242: step 632, loss 0.697582\n",
      "2015-11-22T21:12:19.990031: step 633, loss 0.694328\n",
      "2015-11-22T21:12:25.770415: step 634, loss 0.689085\n",
      "2015-11-22T21:12:31.572449: step 635, loss 0.690916\n",
      "2015-11-22T21:12:37.255624: step 636, loss 0.688362\n",
      "2015-11-22T21:12:43.148591: step 637, loss 0.689165\n",
      "2015-11-22T21:12:49.041711: step 638, loss 0.705357\n",
      "2015-11-22T21:12:54.833692: step 639, loss 0.708754\n",
      "2015-11-22T21:13:00.642555: step 640, loss 0.703548\n",
      "2015-11-22T21:13:06.377816: step 641, loss 0.693175\n",
      "2015-11-22T21:13:12.162463: step 642, loss 0.697041\n",
      "2015-11-22T21:13:17.932854: step 643, loss 0.694004\n",
      "2015-11-22T21:13:23.819199: step 644, loss 0.693729\n",
      "2015-11-22T21:13:29.650119: step 645, loss 0.693095\n",
      "2015-11-22T21:13:35.471063: step 646, loss 0.698898\n",
      "2015-11-22T21:13:41.183870: step 647, loss 0.700534\n",
      "2015-11-22T21:13:46.893763: step 648, loss 0.692836\n",
      "2015-11-22T21:13:52.622191: step 649, loss 0.686286\n",
      "2015-11-22T21:13:58.573806: step 650, loss 0.697443\n",
      "2015-11-22T21:14:04.457273: step 651, loss 0.698206\n",
      "2015-11-22T21:14:10.192889: step 652, loss 0.692959\n",
      "2015-11-22T21:14:15.874740: step 653, loss 0.694275\n",
      "2015-11-22T21:14:21.737662: step 654, loss 0.69548\n",
      "2015-11-22T21:14:27.710505: step 655, loss 0.69607\n",
      "2015-11-22T21:14:33.536500: step 656, loss 0.695472\n",
      "2015-11-22T21:14:39.280160: step 657, loss 0.689881\n",
      "2015-11-22T21:14:45.197704: step 658, loss 0.697394\n",
      "2015-11-22T21:14:50.702732: step 659, loss 0.694333\n",
      "2015-11-22T21:14:56.683738: step 660, loss 0.696537\n",
      "2015-11-22T21:15:03.246700: step 661, loss 0.70046\n",
      "2015-11-22T21:15:09.491256: step 662, loss 0.692296\n",
      "2015-11-22T21:15:15.409333: step 663, loss 0.699352\n",
      "2015-11-22T21:15:21.509859: step 664, loss 0.690141\n",
      "2015-11-22T21:15:27.427313: step 665, loss 0.69066\n",
      "2015-11-22T21:15:33.291488: step 666, loss 0.697995\n",
      "2015-11-22T21:15:39.274460: step 667, loss 0.691556\n",
      "2015-11-22T21:15:45.157183: step 668, loss 0.688235\n",
      "2015-11-22T21:15:51.099124: step 669, loss 0.71867\n",
      "2015-11-22T21:15:57.000926: step 670, loss 0.704348\n",
      "2015-11-22T21:16:02.768054: step 671, loss 0.702007\n",
      "2015-11-22T21:16:08.724288: step 672, loss 0.699404\n",
      "2015-11-22T21:16:14.536220: step 673, loss 0.696544\n",
      "2015-11-22T21:16:20.387660: step 674, loss 0.694831\n",
      "2015-11-22T21:16:26.316535: step 675, loss 0.691569\n",
      "2015-11-22T21:16:32.380325: step 676, loss 0.694675\n",
      "2015-11-22T21:16:38.194533: step 677, loss 0.690119\n",
      "2015-11-22T21:16:43.939019: step 678, loss 0.695856\n",
      "2015-11-22T21:16:49.771647: step 679, loss 0.699856\n",
      "2015-11-22T21:16:55.683357: step 680, loss 0.695059\n",
      "2015-11-22T21:17:01.534763: step 681, loss 0.701397\n",
      "2015-11-22T21:17:07.455263: step 682, loss 0.685496\n",
      "2015-11-22T21:17:13.335424: step 683, loss 0.697436\n",
      "2015-11-22T21:17:19.302971: step 684, loss 0.695034\n",
      "2015-11-22T21:17:25.133961: step 685, loss 0.692653\n",
      "2015-11-22T21:17:30.884376: step 686, loss 0.694897\n",
      "2015-11-22T21:17:36.911654: step 687, loss 0.697765\n",
      "2015-11-22T21:17:42.870822: step 688, loss 0.699689\n",
      "2015-11-22T21:17:48.693423: step 689, loss 0.696743\n",
      "2015-11-22T21:17:54.462032: step 690, loss 0.690057\n",
      "2015-11-22T21:18:00.234677: step 691, loss 0.700628\n",
      "2015-11-22T21:18:05.988410: step 692, loss 0.707406\n",
      "2015-11-22T21:18:11.782192: step 693, loss 0.691353\n",
      "2015-11-22T21:18:17.553153: step 694, loss 0.692791\n",
      "2015-11-22T21:18:23.412408: step 695, loss 0.696288\n",
      "2015-11-22T21:18:29.346685: step 696, loss 0.695059\n",
      "2015-11-22T21:18:37.089365: step 697, loss 0.701891\n",
      "2015-11-22T21:18:42.865463: step 698, loss 0.694388\n",
      "2015-11-22T21:18:50.676286: step 699, loss 0.695685\n",
      "2015-11-22T21:18:56.141205: step 700, loss 0.6893\n",
      "2015-11-22T21:19:01.206223: step 701, loss 0.693893\n",
      "2015-11-22T21:19:06.189271: step 702, loss 0.69533\n",
      "2015-11-22T21:19:11.314580: step 703, loss 0.689574\n",
      "2015-11-22T21:19:16.747502: step 704, loss 0.692064\n",
      "2015-11-22T21:19:22.098071: step 705, loss 0.697067\n",
      "2015-11-22T21:19:29.853929: step 706, loss 0.693999\n",
      "2015-11-22T21:19:36.617264: step 707, loss 0.69683\n",
      "2015-11-22T21:19:42.128634: step 708, loss 0.690554\n",
      "2015-11-22T21:19:49.478247: step 709, loss 0.693706\n",
      "2015-11-22T21:19:56.503223: step 710, loss 0.6922\n",
      "2015-11-22T21:20:01.780632: step 711, loss 0.699897\n",
      "2015-11-22T21:20:06.909321: step 712, loss 0.692269\n",
      "2015-11-22T21:20:12.402402: step 713, loss 0.695346\n",
      "2015-11-22T21:20:18.197253: step 714, loss 0.694717\n",
      "2015-11-22T21:20:23.906726: step 715, loss 0.69299\n",
      "2015-11-22T21:20:31.102770: step 716, loss 0.693302\n",
      "2015-11-22T21:20:37.688878: step 717, loss 0.697323\n",
      "2015-11-22T21:20:45.104381: step 718, loss 0.688898\n",
      "2015-11-22T21:20:50.936415: step 719, loss 0.687668\n",
      "2015-11-22T21:20:56.839292: step 720, loss 0.692211\n",
      "2015-11-22T21:21:02.949191: step 721, loss 0.696188"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Instantiate our model\n",
    "        rnn = CharRNN(VOCABULARY_SIZE, NUM_STEPS, BATCH_SIZE, 2,\n",
    "                      embedding_size=EMBEDDING_SIZE)\n",
    "\n",
    "        # Generate input batches (using tensorflow)\n",
    "        with tf.variable_scope(\"input\"):\n",
    "            placeholder_x = tf.placeholder(tf.int32, train_x.shape)\n",
    "            placeholder_y = tf.placeholder(tf.float32, train_y.shape)\n",
    "            train_x_var = tf.Variable(placeholder_x, trainable=False, collections=[])\n",
    "            train_y_var = tf.Variable(placeholder_y, trainable=False, collections=[])\n",
    "            x_slice, y_slice = tf.train.slice_input_producer([train_x_var, train_y_var], num_epochs=NUM_EPOCHS)\n",
    "            x_batch, y_batch = tf.train.batch([x_slice, y_slice], batch_size=BATCH_SIZE)\n",
    "\n",
    "        # Define Training procedure\n",
    "        out_dir = os.path.join(os.path.curdir, \"runs\", str(int(time.time())))\n",
    "        global_step = tf.Variable(0, name=\"global_step\")\n",
    "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        # optimizer = tf.train.GradientDescentOptimizer(1e-2)\n",
    "        # Clip the gradients\n",
    "        tvars = tf.trainable_variables()\n",
    "        # grads, _ = tf.clip_by_global_norm(tf.gradients(rnn.loss, tvars), 5)\n",
    "        grads = tf.gradients(rnn.loss, tvars)\n",
    "        for g, v in zip(grads, tvars):\n",
    "            if g is not None:\n",
    "                tf.histogram_summary(\"{}/grad\".format(v.name), g)\n",
    "                tf.scalar_summary(\"{}/grad-sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    \n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step)\n",
    "        summary_op = tf.merge_all_summaries()\n",
    "        \n",
    "        # Summaries\n",
    "        train_summary_dir = os.path.abspath(os.path.join(out_dir, \"summaries\", \"train\"))\n",
    "        print(train_summary_dir)\n",
    "        train_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph_def)            \n",
    "        \n",
    "        # Generate train and eval seps\n",
    "#         train_step = rnn.build_train_step(\n",
    "#             out_dir, train_op, global_step, rnn.summaries, ops=[rnn.final_state], save_every=8, sess=sess)\n",
    "#         eval_step = rnn.build_eval_step(out_dir, rnn.predictions, global_step, rnn.summaries, sess=sess)\n",
    "\n",
    "        # Initialize variables and input data\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        sess.run(\n",
    "            [train_x_var.initializer, train_y_var.initializer],\n",
    "            {placeholder_x: train_x, placeholder_y: train_y})\n",
    "\n",
    "        # Initialize queues\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        # Print model parameters\n",
    "        # rnn.print_parameters()\n",
    "        \n",
    "        def rolled_batches(x_batch, y_batch, gains):\n",
    "            num_unrolls = SENTENCE_LENGTH_PADDED/NUM_STEPS\n",
    "            x_unrolls = np.split(x_batch, num_unrolls, 1)\n",
    "            gain_unrolls = np.split(gains, num_unrolls)\n",
    "            for x_unroll, gain_unroll in zip(x_unrolls, gain_unrolls):\n",
    "                feed_dict = {\n",
    "                    rnn.input_x: x_unroll,\n",
    "                    rnn.input_y: y_batch,\n",
    "                    rnn.anneal_factors: gain_unroll\n",
    "                }\n",
    "                yield feed_dict\n",
    "                \n",
    "        def eval_dev(dev_x, dev_y):\n",
    "            drop_num_elements = len(dev_y) % BATCH_SIZE\n",
    "            if drop_num_elements > 0:\n",
    "                dev_x_ = dev_x[:-drop_num_elements]\n",
    "                dev_y_ = dev_y[:-drop_num_elements]\n",
    "            nbatches = len(dev_y)/BATCH_SIZE\n",
    "            predictions = []\n",
    "            # For each batch...\n",
    "            for batch_x, batch_y in zip(np.split(dev_x_, nbatches), np.split(dev_y_, nbatches)):\n",
    "                gains = np.zeros(SENTENCE_LENGTH_PADDED) # Not used\n",
    "                state = np.zeros(rnn.initial_state.get_shape().as_list())\n",
    "                # For each unroll step...\n",
    "                for feed_dict in rolled_batches(batch_x, batch_y, gains):\n",
    "                    feed_dict[rnn.initial_state] = state\n",
    "                    batch_predictions, state = sess.run([rnn.predictions, rnn.final_state], feed_dict)\n",
    "                predictions = np.append(predictions, batch_predictions)\n",
    "            print(classification_report(np.argmax(dev_y_, axis=1), predictions))\n",
    "            \n",
    "        def train_step(batch_x, batch_y):\n",
    "            state = np.zeros(rnn.initial_state.get_shape().as_list())\n",
    "            # gains = np.linspace(0.0, 1.0, SENTENCE_LENGTH_PADDED)\n",
    "            # Only consider the last loss\n",
    "            gains = np.zeros(SENTENCE_LENGTH_PADDED)\n",
    "            gains[-1] = 1.0\n",
    "            # We unroll the nework several times and pass the state\n",
    "            for feed_dict in rolled_batches(batch_x, batch_y, gains):\n",
    "                feed_dict[rnn.initial_state] = state\n",
    "                _, state, loss, global_step_, summaries_ = sess.run(\n",
    "                    [train_op, rnn.final_state, rnn.loss, global_step, summary_op],\n",
    "                    feed_dict=feed_dict)\n",
    "            train_writer.add_summary(summaries_, global_step_)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}\".format(time_str, global_step_, loss))\n",
    "            \n",
    "        # Repeat until we're done (the input queue throws an error)...\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                batch_x = x_batch.eval()\n",
    "                batch_y = y_batch.eval()\n",
    "                train_step(batch_x, batch_y)\n",
    "                if global_step.eval() % EVALUATE_EVERY == 0:\n",
    "                    # eval_dev(dev_x, dev_y)\n",
    "                    pass\n",
    "                    \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"Yay, training done!\")\n",
    "            eval_step({rnn.input_x: dev_x, rnn.input_y: dev_y})\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
