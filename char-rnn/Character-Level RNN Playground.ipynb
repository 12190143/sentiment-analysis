{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "import time\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "import utils.ymr_data as ymr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Model Hyperparameters\n",
    "SENTENCE_LENGTH_PADDED = int(os.getenv(\"SENTENCE_LENGTH_PADDED\", \"100\"))\n",
    "HIDDEN_DIM = int(os.getenv(\"HIDDEN_DIM\", \"128\"))\n",
    "EMBEDDING_SIZE = int(os.getenv(\"EMBEDDING_SIZE\", \"128\"))\n",
    "\n",
    "# Training parameters\n",
    "LEARNING_RATE = float(os.getenv(\"LEARNING_RATE\", \"1e-4\"))\n",
    "NUM_EPOCHS = int(os.getenv(\"NUM_EPOCHS\", \"100\"))\n",
    "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"16\"))\n",
    "EVALUATE_EVERY = int(os.getenv(\"EVALUATE_EVERY\", \"16\"))\n",
    "\n",
    "# Output files\n",
    "RUNDIR = \"./runs/%s\" % int(time.time())\n",
    "CHECKPOINT_PREFIX = os.getenv(\"CHECKPOINT_PREFIX\", \"%s/checkpoints/char-cnn\" % RUNDIR)\n",
    "TRAIN_SUMMARY_DIR = os.getenv(\"TRAIN_SUMMARY_DIR\", \"%s/summaries/train\" % RUNDIR)\n",
    "DEV_SUMMARY_DIR = os.getenv(\"TRAIN_SUMMARY_DIR\", \"%s/summaries/dev\" % RUNDIR)\n",
    "\n",
    "# Misc Parameters\n",
    "NUM_GPUS = int(os.getenv(\"NUM_GPUS\", \"4\"))\n",
    "ALLOW_SOFT_PLACEMENT = bool(os.getenv(\"ALLOW_SOFT_PLACEMENT\", 1))\n",
    "LOG_DEVICE_PLACEMENT = bool(os.getenv(\"LOG_DEVICE_PLACEMENT\", 0))\n",
    "PADDING_CHARACTER = u\"\\u0000\"\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "if not os.path.exists(os.path.dirname(CHECKPOINT_PREFIX)):\n",
    "    os.makedirs(os.path.dirname(CHECKPOINT_PREFIX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Size\n",
      "----------\n",
      "Training set size: 29017\n",
      "Dev set size: 1528\n",
      "Test set size: 7637\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "# ==================================================\n",
    "df = ymr.load()\n",
    "\n",
    "# Preprocessing: Pad all sentences\n",
    "df.text = df.text.str.slice(0, SENTENCE_LENGTH_PADDED).str.ljust(\n",
    "    SENTENCE_LENGTH_PADDED, PADDING_CHARACTER)\n",
    "\n",
    "# Generate vocabulary and dataset\n",
    "vocab, vocab_inv = ymr.vocab(df)\n",
    "data = ymr.make_polar(df)\n",
    "train, test = ymr.train_test_split(data)\n",
    "train_x, train_y_ = ymr.make_xy(train, vocab)\n",
    "test_x, test_y_ = ymr.make_xy(test, vocab)\n",
    "\n",
    "VOCABULARY_SIZE = len(vocab)\n",
    "\n",
    "# Convert ys to one-hot vectors (probability distribution)\n",
    "train_y = np.zeros((len(train_y_), NUM_CLASSES))\n",
    "train_y[np.arange(len(train_y_)), train_y_] = 1.\n",
    "test_y = np.zeros((len(test_y_), NUM_CLASSES))\n",
    "test_y[np.arange(len(test_y_)), test_y_] = 1.\n",
    "\n",
    "# Use a dev set\n",
    "train_x, dev_x, train_y, dev_y = train_test_split(\n",
    "    train_x, train_y, test_size=0.05)\n",
    "\n",
    "# Print data sizes\n",
    "print(\"\\nData Size\")\n",
    "print(\"----------\")\n",
    "print(\"Training set size: %d\" % (len(train_y)))\n",
    "print(\"Dev set size: %d\" % len(dev_y))\n",
    "print(\"Test set size: %d\" % len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    # Build the graph\n",
    "    # ==================================================\n",
    "    # Keeps track of shapes, for debugging purposes\n",
    "    shape_tensors = []\n",
    "\n",
    "    def debug_shape(name, tensor):\n",
    "        full_name = \"%s-shape\" % name\n",
    "        shape_tensors.append(tf.shape(tensor, name=full_name))\n",
    "        \n",
    "    # Input data\n",
    "    # --------------------------------------------------\n",
    "    # Store the data in graph notes\n",
    "    train_x_const = tf.constant(train_x.astype(\"int32\"))\n",
    "    train_y_const = tf.constant(train_y.astype(\"float32\"))\n",
    "    # Use Tensorflow's queues and batching features\n",
    "    x_slice, y_slice = tf.train.slice_input_producer(\n",
    "        [train_x_const, train_y_const],\n",
    "        num_epochs=NUM_EPOCHS)\n",
    "    x, y_ = tf.train.batch([x_slice, y_slice], batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Layer 1: Embedding\n",
    "    # --------------------------------------------------\n",
    "    # Not supported by GPU...\n",
    "    with tf.device('/cpu:0'):\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            W_embeddings = tf.Variable(\n",
    "                tf.random_uniform([VOCABULARY_SIZE, EMBEDDING_SIZE], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            embed = tf.nn.embedding_lookup(W_embeddings, x)\n",
    "            # Add a dimension corresponding to the channel - it's expected by the conv\n",
    "            # layer\n",
    "            debug_shape(\"W\", embed)\n",
    "\n",
    "    embedded_characters = tf.split(1, SENTENCE_LENGTH_PADDED, embed)\n",
    "    embedded_characters_list = [tf.reshape(w, [BATCH_SIZE, EMBEDDING_SIZE]) for w in embedded_characters]\n",
    "\n",
    "    # RNN Layer\n",
    "    with tf.name_scope(\"lstm\"):\n",
    "        lstm_cell = rnn_cell.BasicLSTMCell(HIDDEN_DIM, forget_bias=0.0)\n",
    "        cell = rnn_cell.MultiRNNCell([lstm_cell] * 5)\n",
    "        initial_state = cell.zero_state(BATCH_SIZE, tf.float32)\n",
    "        outputs, states = rnn.rnn(cell, embedded_characters_list, initial_state=initial_state)\n",
    "        last_output = outputs[-1]\n",
    "        debug_shape(\"last_output\", outputs[-1])\n",
    "\n",
    "    with tf.name_scope(\"softmax\"):\n",
    "        W_softmax = tf.Variable(tf.truncated_normal(\n",
    "                [HIDDEN_DIM, NUM_CLASSES], stddev=0.1), name=\"W\")\n",
    "        b_softmax = tf.Variable(\n",
    "            tf.constant(0.1, shape=[NUM_CLASSES]), name=\"b\")\n",
    "        y = tf.nn.softmax(\n",
    "            tf.matmul(last_output, W_softmax) + b_softmax, name=\"y\")\n",
    "    \n",
    "    with tf.name_scope(\"loss\"):\n",
    "        ce_loss_mean = -tf.reduce_mean(y_ * tf.log(y), name=\"ce_loss_mean\")\n",
    "    \n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_predictions = tf.equal(\n",
    "            tf.argmax(y, 1), tf.argmax(y_, 1), name=\"correct_predictions\")\n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(ce_loss_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_shapes():\n",
    "    \"\"\"\n",
    "    Prints the shapes of the graph for one batch\n",
    "    \"\"\"\n",
    "    sess = tf.get_default_session()\n",
    "    feed_dict = {x: train_x[:BATCH_SIZE], y_: train_y[:BATCH_SIZE]}\n",
    "    shapes = sess.run(shape_tensors, feed_dict=feed_dict)\n",
    "    print(\"\\nShapes\")\n",
    "    print(\"----------\")\n",
    "    for k, v in zip(shape_tensors, shapes):\n",
    "        print(\"%s: %s\" % (k.name, v))\n",
    "        \n",
    "def train_batch(step):\n",
    "    \"\"\"\n",
    "    Trains a single batch\n",
    "    \"\"\"\n",
    "    sess = tf.get_default_session()\n",
    "    # feed_dict = {x: batch_x, y_: batch_y}\n",
    "    _, train_loss, train_accuracy, train_summary_str = sess.run(\n",
    "        [train_step, ce_loss_mean, accuracy, summary_op])\n",
    "    summary_writer_train.add_summary(train_summary_str, step)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "----------\n",
      "input_producer/input_producer/limit_epochs/epochs:0: 1\n",
      "embedding/W:0: 468,096\n",
      "RNN/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Matrix:0: 131,072\n",
      "RNN/MultiRNNCell/Cell0/BasicLSTMCell/Linear/Bias:0: 512\n",
      "RNN/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Matrix:0: 131,072\n",
      "RNN/MultiRNNCell/Cell1/BasicLSTMCell/Linear/Bias:0: 512\n",
      "RNN/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Matrix:0: 131,072\n",
      "RNN/MultiRNNCell/Cell2/BasicLSTMCell/Linear/Bias:0: 512\n",
      "RNN/MultiRNNCell/Cell3/BasicLSTMCell/Linear/Matrix:0: 131,072\n",
      "RNN/MultiRNNCell/Cell3/BasicLSTMCell/Linear/Bias:0: 512\n",
      "RNN/MultiRNNCell/Cell4/BasicLSTMCell/Linear/Matrix:0: 131,072\n",
      "RNN/MultiRNNCell/Cell4/BasicLSTMCell/Linear/Bias:0: 512\n",
      "softmax/W:0: 256\n",
      "softmax/b:0: 2\n",
      "beta1_power:0: 1\n",
      "beta2_power:0: 1\n",
      "\n",
      "Total Parameters: 1,126,277\n",
      "\n",
      "\n",
      "Shapes\n",
      "----------\n",
      "embedding/W-shape:0: [ 16 100 128]\n",
      "lstm/last_output-shape:0: [ 16 128]\n"
     ]
    }
   ],
   "source": [
    "with g.as_default():\n",
    "    # Print parameters\n",
    "    print \"\\nParameters:\"\n",
    "    print(\"----------\")\n",
    "    total_parameters = 0\n",
    "    for v in tf.trainable_variables():\n",
    "        num_parameters = v.get_shape().num_elements()\n",
    "        print(\"{}: {:,}\".format(v.name, num_parameters))\n",
    "        total_parameters += num_parameters\n",
    "    print(\"\\nTotal Parameters: {:,}\\n\".format(total_parameters))\n",
    "\n",
    "# Write graph\n",
    "tf.train.write_graph(g.as_graph_def(), \"%s/graph\" % RUNDIR, \"graph.pb\", as_text=False)\n",
    "\n",
    "# Initialize training\n",
    "step = 0\n",
    "\n",
    "session_config = tf.ConfigProto(\n",
    "    log_device_placement=LOG_DEVICE_PLACEMENT,\n",
    "    allow_soft_placement=ALLOW_SOFT_PLACEMENT)\n",
    "\n",
    "with tf.Session(graph=g, config=session_config) as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    # Initialize queue runners\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    print_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
