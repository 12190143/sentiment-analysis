{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import utils.ymr_data as ymr\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "SENTENCE_LENGTH_PADDED=512\n",
    "EMBEDDING_SIZE = 50\n",
    "BATCH_SIZE=16\n",
    "L1_NUM_FILTERS = 50\n",
    "TRAIN_SUMMARY_DIR = \"./summaries/train\"\n",
    "DEV_SUMMARY_DIR = \"./summaries/dev\"\n",
    "\n",
    "PADDING_CHARACTER =  u\"\\u0000\"\n",
    "NUM_CLASSES=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = ymr.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocessing: Pad all sentences\n",
    "df.text = df.text.str.slice(0,SENTENCE_LENGTH_PADDED).str.ljust(SENTENCE_LENGTH_PADDED, PADDING_CHARACTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/pandas/core/indexing.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Generate vocabulary and dataset\n",
    "vocab, vocab_inv = ymr.vocab(df)\n",
    "data = ymr.make_polar(df)\n",
    "train, test = ymr.train_test_split(data)\n",
    "train_x, train_y_ = ymr.make_xy(train, vocab)\n",
    "test_x, test_y_ = ymr.make_xy(test, vocab)\n",
    "\n",
    "VOCABULARY_SIZE = len(vocab)\n",
    "\n",
    "# Convert ys to probability distribution\n",
    "train_y = np.zeros((len(train_y_), NUM_CLASSES))\n",
    "train_y[np.arange(len(train_y_)), train_y_] = 1.\n",
    "test_y = np.zeros((len(test_y_), NUM_CLASSES))\n",
    "test_y[np.arange(len(test_y_)), test_y_] = 1.\n",
    "\n",
    "# Use a dev set\n",
    "train_x, dev_x, train_y, dev_y = train_test_split(train_x, train_y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 29017\n",
      "Dev set size: 1528\n",
      "Test set size: 7637\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set size: %d\" % len(train_y))\n",
    "print(\"Dev set size: %d\" % len(dev_y))\n",
    "print(\"Test set size: %d\" % len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "# ==================================================\n",
    "\n",
    "# Network inputs and output\n",
    "x = tf.placeholder(tf.int32, shape=[None, SENTENCE_LENGTH_PADDED], name=\"x\")\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, NUM_CLASSES], name=\"y\")\n",
    "\n",
    "# Variables\n",
    "W_embeddings = tf.Variable(tf.random_uniform([VOCABULARY_SIZE, EMBEDDING_SIZE], -1.0, 1.0), name=\"W_embeddings\")\n",
    "\n",
    "# Layer 1: Embedding\n",
    "embed = tf.nn.embedding_lookup(W_embeddings, x)\n",
    "# Add a dimension corresponding to the channel - it's expected by the conv layer\n",
    "embed_expanded = tf.expand_dims(embed, -1)\n",
    "embed_shape = tf.shape(embed_expanded)\n",
    "\n",
    "# Layer 2: Simple Convolutional Layer\n",
    "\n",
    "W_conv1 = tf.Variable(tf.truncated_normal([3, EMBEDDING_SIZE, 1, L1_NUM_FILTERS], stddev=0.1), name=\"W_conv1\")\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[L1_NUM_FILTERS]), name=\"b_conv1\")\n",
    "h_conv1_tmp = tf.nn.conv2d(embed_expanded, W_conv1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "h_conv1 = tf.nn.relu(h_conv1_tmp + b_conv1)\n",
    "h_conv1_shape = tf.shape(h_conv1_tmp)\n",
    "\n",
    "# Layer 3: Max-pooling\n",
    "h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, SENTENCE_LENGTH_PADDED, EMBEDDING_SIZE, 1], strides=[1, 1, 1, 1], padding='VALID')\n",
    "h_pool1_shape = tf.shape(h_pool1)\n",
    "\n",
    "# Layer 4: Fully connected\n",
    "h_pool1_flat = tf.reshape(h_pool1, [-1, L1_NUM_FILTERS])\n",
    "h_pool1_flat_shape = tf.shape(h_pool1_flat)\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([L1_NUM_FILTERS, 256], stddev=0.1), name=\"W_fc1\")\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape=[256]), name=\"b_fc1\")\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool1_flat, W_fc1) + b_fc1)\n",
    "h_fc1_shape = tf.shape(h_fc1)\n",
    "\n",
    "# TODO: Dropout?\n",
    "\n",
    "# Layer 5: Softmax / Readout\n",
    "W_fc2 = tf.Variable(tf.truncated_normal([256, NUM_CLASSES], stddev=0.1), name=\"W_fc2\")\n",
    "b_fc2 =  tf.Variable(tf.constant(0.1, shape=[NUM_CLASSES]), name=\"b_fc2\")\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "y_conv_shape = tf.shape(y_conv)\n",
    "\n",
    "# Training procedure\n",
    "cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv), name=\"crossentropy_sum\")\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"), name=\"accuracy\")\n",
    "\n",
    "# Summaries\n",
    "ce_summary = tf.scalar_summary(\"cross-entropy\", cross_entropy)\n",
    "accuracy_summary = tf.scalar_summary(\"accuracy\", accuracy)\n",
    "summary_op = tf.merge_all_summaries()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SPLIT_SIZE = int(len(train_x)/BATCH_SIZE)\n",
    "train_x_batched = np.array_split(train_x, SPLIT_SIZE)\n",
    "train_y_batched = np.array_split(train_y, SPLIT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train loss: 77.4975\n",
      "step 0, dev loss 285.494\n",
      "step 0, dev accuracy 0\n",
      "step 1, train loss: 73.8183\n",
      "step 2, train loss: 74.5399\n",
      "step 3, train loss: 71.4919\n",
      "step 4, train loss: 67.5844\n",
      "step 5, train loss: 67.0926\n",
      "step 6, train loss: 66.0343\n",
      "step 7, train loss: 59.6602\n",
      "step 8, train loss: 60.4036\n",
      "step 9, train loss: 60.0504\n",
      "step 10, train loss: 55.1207\n",
      "step 11, train loss: 50.59\n",
      "step 12, train loss: 48.3509\n",
      "step 13, train loss: 46.3553\n",
      "step 14, train loss: 45.7437\n",
      "step 15, train loss: 47.0966\n",
      "step 16, train loss: 46.2927\n",
      "step 16, dev loss 168.5\n",
      "step 16, dev accuracy 0\n",
      "step 17, train loss: 39.437\n",
      "step 18, train loss: 40.7202\n",
      "step 19, train loss: 37.9014\n",
      "step 20, train loss: 39.6059\n",
      "step 21, train loss: 37.362\n",
      "step 22, train loss: 35.8069\n",
      "step 23, train loss: 34.9371\n",
      "step 24, train loss: 30.8921\n",
      "step 25, train loss: 32.1975\n",
      "step 26, train loss: 30.1022\n",
      "step 27, train loss: 31.2146\n",
      "step 28, train loss: 29.217\n",
      "step 29, train loss: 25.844\n",
      "step 30, train loss: 26.2879\n",
      "step 31, train loss: 23.4961\n",
      "step 32, train loss: 22.168\n",
      "step 32, dev loss 90.8406\n",
      "step 32, dev accuracy 0.515625\n",
      "step 33, train loss: 19.9398\n",
      "step 34, train loss: 22.6545\n",
      "step 35, train loss: 21.654\n",
      "step 36, train loss: 20.1411\n",
      "step 37, train loss: 19.6215\n",
      "step 38, train loss: 20.4085\n",
      "step 39, train loss: 18.0217\n",
      "step 40, train loss: 17.7438\n",
      "step 41, train loss: 17.2919\n",
      "step 42, train loss: 17.8307\n",
      "step 43, train loss: 16.4024\n",
      "step 44, train loss: 15.8954\n",
      "step 45, train loss: 17.1305\n",
      "step 46, train loss: 15.0486\n",
      "step 47, train loss: 15.7421\n",
      "step 48, train loss: 16.9693\n",
      "step 48, dev loss 61.0888\n",
      "step 48, dev accuracy 0.484375\n",
      "step 49, train loss: 14.8652\n",
      "step 50, train loss: 14.3514\n",
      "step 51, train loss: 16.374\n",
      "step 52, train loss: 15.0064\n",
      "step 53, train loss: 14.9632\n",
      "step 54, train loss: 14.521\n",
      "step 55, train loss: 13.8643\n",
      "step 56, train loss: 13.7997\n",
      "step 57, train loss: 13.6377\n",
      "step 58, train loss: 13.7535\n",
      "step 59, train loss: 13.4636\n",
      "step 60, train loss: 13.3767\n",
      "step 61, train loss: 13.827\n",
      "step 62, train loss: 13.1976\n",
      "step 63, train loss: 13.0817\n",
      "step 64, train loss: 13.6763\n",
      "step 64, dev loss 53.0947\n",
      "step 64, dev accuracy 0.515625\n",
      "step 65, train loss: 13.3573\n",
      "step 66, train loss: 13.3323\n",
      "step 67, train loss: 12.4683\n",
      "step 68, train loss: 12.3567\n",
      "step 69, train loss: 11.9591\n",
      "step 70, train loss: 12.743\n",
      "step 71, train loss: 13.0119\n",
      "step 72, train loss: 12.1693\n",
      "step 73, train loss: 13.1949\n",
      "step 74, train loss: 12.8005\n",
      "step 75, train loss: 12.4368\n",
      "step 76, train loss: 13.6256\n",
      "step 77, train loss: 12.5647\n",
      "step 78, train loss: 12.8227\n",
      "step 79, train loss: 12.2796\n",
      "step 80, train loss: 12.0532\n",
      "step 80, dev loss 49.9654\n",
      "step 80, dev accuracy 0.484375\n",
      "step 81, train loss: 12.3685\n",
      "step 82, train loss: 12.1661\n",
      "step 83, train loss: 12.5785\n",
      "step 84, train loss: 11.5473\n",
      "step 85, train loss: 12.274\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-55e218f38299>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_batched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_x_batched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtrain_y_batched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"step %d, train loss: %g\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mtrain_summary_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/projects/python-venvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/projects/python-venvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 404\u001b[0;31m                                target_list)\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EVALUATE_DEV_EVERY=16\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    summary_writer_train = tf.train.SummaryWriter(TRAIN_SUMMARY_DIR, graph_def=sess.graph_def)\n",
    "    summary_writer_dev = tf.train.SummaryWriter(DEV_SUMMARY_DIR, graph_def=sess.graph_def)\n",
    "    saver = tf.train.Saver()\n",
    "    step = 0\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        for i in range(len(train_x_batched)):\n",
    "            feed_dict = { x: train_x_batched[i], y_ : train_y_batched[i]}\n",
    "            _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "            print(\"step %d, train loss: %g\" % (step, loss))\n",
    "            train_summary_str = sess.run(summary_op, feed_dict=feed_dict)\n",
    "            summary_writer_train.add_summary(train_summary_str, step)\n",
    "            if(step % EVALUATE_DEV_EVERY == 0):\n",
    "                feed_dict = { x: dev_x[:64], y_ :dev_y[:64]}\n",
    "                dev_loss, dev_accuracy, dev_summary_str = sess.run([cross_entropy, accuracy, summary_op], feed_dict=feed_dict)\n",
    "                summary_writer_dev.add_summary(dev_summary_str, step)\n",
    "                print \"step %d, dev loss %g\"%(step, dev_loss)\n",
    "                print \"step %d, dev accuracy %g\"%(step, dev_accuracy)\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
