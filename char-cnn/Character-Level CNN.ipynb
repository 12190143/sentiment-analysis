{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import utils.ymr_data as ymr\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import math\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "SENTENCE_LENGTH_PADDED=512\n",
    "EMBEDDING_SIZE = 150\n",
    "BATCH_SIZE=128\n",
    "EVALUATE_DEV_EVERY=16\n",
    "L1_NUM_FILTERS = 150\n",
    "L1_FILTER_SIZES = [2,3,4]\n",
    "CHECKPOINTS_DIR = \"./checkpoints/\"\n",
    "TRAIN_SUMMARY_DIR = \"./summaries/train\"\n",
    "DEV_SUMMARY_DIR = \"./summaries/dev\"\n",
    "\n",
    "PADDING_CHARACTER =  u\"\\u0000\"\n",
    "NUM_CLASSES=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = ymr.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocessing: Pad all sentences\n",
    "df.text = df.text.str.slice(0,SENTENCE_LENGTH_PADDED).str.ljust(SENTENCE_LENGTH_PADDED, PADDING_CHARACTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate vocabulary and dataset\n",
    "vocab, vocab_inv = ymr.vocab(df)\n",
    "data = ymr.make_polar(df)\n",
    "train, test = ymr.train_test_split(data)\n",
    "train_x, train_y_ = ymr.make_xy(train, vocab)\n",
    "test_x, test_y_ = ymr.make_xy(test, vocab)\n",
    "\n",
    "VOCABULARY_SIZE = len(vocab)\n",
    "\n",
    "# Convert ys to probability distribution\n",
    "train_y = np.zeros((len(train_y_), NUM_CLASSES))\n",
    "train_y[np.arange(len(train_y_)), train_y_] = 1.\n",
    "test_y = np.zeros((len(test_y_), NUM_CLASSES))\n",
    "test_y[np.arange(len(test_y_)), test_y_] = 1.\n",
    "\n",
    "# Use a dev set\n",
    "train_x, dev_x, train_y, dev_y = train_test_split(train_x, train_y, test_size=0.05)\n",
    "\n",
    "# BATCH\n",
    "SPLIT_SIZE = math.ceil(len(train_x)/float(BATCH_SIZE))\n",
    "train_x_batched = np.array_split(train_x, SPLIT_SIZE)\n",
    "train_y_batched = np.array_split(train_y, SPLIT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 29017\n",
      "Dev set size: 1528\n",
      "Test set size: 7637\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set size: %d\" % len(train_y))\n",
    "print(\"Dev set size: %d\" % len(dev_y))\n",
    "print(\"Test set size: %d\" % len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "# ==================================================\n",
    "\n",
    "shape_variables = []\n",
    "\n",
    "def build_graph():\n",
    "    \n",
    "    # Network inputs and output\n",
    "    x = tf.placeholder(tf.int32, shape=[None, SENTENCE_LENGTH_PADDED], name=\"x\")\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, NUM_CLASSES], name=\"y\")\n",
    "\n",
    "    # Layer 1: Embedding\n",
    "    with tf.name_scope(\"embedding\") as scope:\n",
    "        W_embeddings = tf.Variable(tf.random_uniform([VOCABULARY_SIZE, EMBEDDING_SIZE], -1.0, 1.0), name=\"W_embeddings\")\n",
    "        embed = tf.nn.embedding_lookup(W_embeddings, x)\n",
    "        # Add a dimension corresponding to the channel - it's expected by the conv layer\n",
    "        embed_expanded = tf.expand_dims(embed, -1)\n",
    "        shape_variables.append((\"Embedding\", tf.shape(embed_expanded, name=\"embed_shape\")))\n",
    "    \n",
    "    # Convolutional filters\n",
    "    def build_convpool(filter_size, num_filters):\n",
    "        W = tf.get_variable(\"weights\", [filter_size, EMBEDDING_SIZE, 1, num_filters],\n",
    "                       initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        b = tf.get_variable(\"bias\", [num_filters], initializer=tf.constant_initializer(0.1))\n",
    "        conv_tmp = tf.nn.conv2d(embed_expanded, W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "        h_conv = tf.nn.relu(conv_tmp + b)\n",
    "        pooled = tf.nn.max_pool(h_conv, \n",
    "            ksize=[1, SENTENCE_LENGTH_PADDED-filter_size+1, 1, 1],\n",
    "            strides=[1, 1, 1, 1], padding='VALID')\n",
    "        return pooled\n",
    "    \n",
    "    pooled_outputs = []\n",
    "    total_filters = L1_NUM_FILTERS * len(L1_FILTER_SIZES)\n",
    "    for filter_size in L1_FILTER_SIZES:\n",
    "        with tf.variable_scope(\"conv-%s\" % filter_size):\n",
    "            # Layer 2: Simple Convolutional Layer\n",
    "            with tf.name_scope(\"conv-%s\" % filter_size):\n",
    "                pooled = build_convpool(filter_size, L1_NUM_FILTERS)\n",
    "                pooled_outputs.append(pooled)\n",
    "                shape_variables.append((\"Pooled Output (%s)\" % filter_size, tf.shape(pooled)))\n",
    "    \n",
    "    # Combine all the pooled features\n",
    "    h_pool = tf.concat(3, pooled_outputs)\n",
    "    shape_variables.append((\"Pooled Output Final\", tf.shape(h_pool)))\n",
    "\n",
    "    # Layer 4: Fully connected\n",
    "    with tf.name_scope(\"affine_1\") as scope: \n",
    "        h_pool1_flat = tf.reshape(h_pool, [-1, total_filters])\n",
    "        W_fc1 = tf.Variable(tf.truncated_normal([total_filters, 256], stddev=0.1), name=\"W_fc1\")\n",
    "        b_fc1 = tf.Variable(tf.constant(0.1, shape=[256]), name=\"b_fc1\")\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool1_flat, W_fc1) + b_fc1)\n",
    "        shape_variables.append((\"Pooled Flat\", tf.shape(h_pool1_flat)))\n",
    "        shape_variables.append((\"Affine\", tf.shape(h_fc1)))\n",
    "\n",
    "    # Dropout\n",
    "    with tf.name_scope(\"dropout1\") as scope: \n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, 0.5)\n",
    "\n",
    "    # Layer 5: Softmax / Readout\n",
    "    with tf.name_scope(\"softmax\") as scope: \n",
    "        W_fc2 = tf.Variable(tf.truncated_normal([256, NUM_CLASSES], stddev=0.1), name=\"W_fc2\")\n",
    "        b_fc2 =  tf.Variable(tf.constant(0.1, shape=[NUM_CLASSES]), name=\"b_fc2\")\n",
    "        y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "    with tf.name_scope(\"loss\") as scope: \n",
    "        cross_entropy = -tf.reduce_mean(y_ * tf.log(y_conv), name=\"crossentropy_sum\")\n",
    "\n",
    "    # Training procedure\n",
    "    with tf.name_scope(\"accuracy\") as scope: \n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"), name=\"accuracy\")\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    # Summaries\n",
    "    ce_summary = tf.scalar_summary(\"cross-entropy\", cross_entropy)\n",
    "    accuracy_summary = tf.scalar_summary(\"accuracy\", accuracy)\n",
    "    summary_op = tf.merge_all_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_batch(batch_x, batch_y, step):\n",
    "    feed_dict = { x: batch_x, y_ : batch_y}\n",
    "    # Train\n",
    "    _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "    # print(\"step %d, train loss: %g\" % (step, loss))\n",
    "    # Summary\n",
    "    train_summary_str = sess.run(summary_op, feed_dict=feed_dict)\n",
    "    summary_writer_train.add_summary(train_summary_str, step)\n",
    "\n",
    "def evaluate_dev(step):\n",
    "    feed_dict = { x: dev_x, y_ :dev_y}\n",
    "    # Evaluate\n",
    "    dev_loss, dev_accuracy, dev_summary_str = sess.run([cross_entropy, accuracy, summary_op], feed_dict=feed_dict)\n",
    "    print \"step %d, dev loss %g\"%(step, dev_loss)\n",
    "    print \"step %d, dev accuracy %g\"%(step, dev_accuracy)\n",
    "    # Write summary\n",
    "    summary_writer_dev.add_summary(dev_summary_str, step)\n",
    "\n",
    "def print_shapes(batch_x, batch_y):\n",
    "    feed_dict = { x: batch_x, y_ : batch_y}\n",
    "    names, vals = zip(*shape_variables)\n",
    "    shapes = sess.run(vals, feed_dict=feed_dict)\n",
    "    print(\"Shapes\")\n",
    "    print(\"-----\")\n",
    "    for k,v in zip(names, shapes):\n",
    "        print(\"%s: %s\" % (k,v))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "-----\n",
      "Embedding: [128 256 150   1]\n",
      "Pooled Output (2): [128   1   1 150]\n",
      "Pooled Output (3): [128   1   1 150]\n",
      "Pooled Output (4): [128   1   1 150]\n",
      "Pooled Output Final: [128   1   1 450]\n",
      "Pooled Flat: [128 450]\n",
      "Affine: [128 256]\n",
      "-----\n",
      "\n",
      "Epoch 0\n",
      "----------\n",
      "step 0, dev loss 1.06273\n",
      "step 0, dev accuracy 0.304974\n",
      "step 16, dev loss 0.48212\n",
      "step 16, dev accuracy 0.507853\n",
      "step 32, dev loss 0.407196\n",
      "step 32, dev accuracy 0.526178\n",
      "step 48, dev loss 0.320066\n",
      "step 48, dev accuracy 0.518979\n",
      "step 64, dev loss 0.272356\n",
      "step 64, dev accuracy 0.510471\n",
      "step 80, dev loss 0.226876\n",
      "step 80, dev accuracy 0.501963\n",
      "step 96, dev loss 0.188967\n",
      "step 96, dev accuracy 0.530759\n",
      "step 112, dev loss 0.161883\n",
      "step 112, dev accuracy 0.545157\n",
      "step 128, dev loss 0.145377\n",
      "step 128, dev accuracy 0.564136\n",
      "step 144, dev loss 0.142027\n",
      "step 144, dev accuracy 0.540576\n",
      "step 160, dev loss 0.129555\n",
      "step 160, dev accuracy 0.554974\n",
      "step 176, dev loss 0.130258\n",
      "step 176, dev accuracy 0.543194\n",
      "step 192, dev loss 0.127207\n",
      "step 192, dev accuracy 0.549084\n",
      "step 208, dev loss 0.122493\n",
      "step 208, dev accuracy 0.570026\n",
      "step 224, dev loss 0.11898\n",
      "step 224, dev accuracy 0.564136\n",
      "\n",
      "Epoch 1\n",
      "----------\n",
      "step 240, dev loss 0.117187\n",
      "step 240, dev accuracy 0.57788\n",
      "step 256, dev loss 0.117341\n",
      "step 256, dev accuracy 0.582461\n",
      "step 272, dev loss 0.117497\n",
      "step 272, dev accuracy 0.573953\n",
      "step 288, dev loss 0.116148\n",
      "step 288, dev accuracy 0.593586\n",
      "step 304, dev loss 0.112511\n",
      "step 304, dev accuracy 0.604058\n",
      "step 320, dev loss 0.113111\n",
      "step 320, dev accuracy 0.600131\n",
      "step 336, dev loss 0.112539\n",
      "step 336, dev accuracy 0.610602\n",
      "step 352, dev loss 0.111779\n",
      "step 352, dev accuracy 0.608639\n",
      "step 368, dev loss 0.110624\n",
      "step 368, dev accuracy 0.617801\n",
      "step 384, dev loss 0.108814\n",
      "step 384, dev accuracy 0.615838\n",
      "step 400, dev loss 0.108449\n",
      "step 400, dev accuracy 0.627618\n",
      "step 416, dev loss 0.109809\n",
      "step 416, dev accuracy 0.617147\n",
      "step 432, dev loss 0.10858\n",
      "step 432, dev accuracy 0.637435\n",
      "step 448, dev loss 0.108826\n",
      "step 448, dev accuracy 0.627618\n",
      "\n",
      "Epoch 2\n",
      "----------\n",
      "step 464, dev loss 0.105625\n",
      "step 464, dev accuracy 0.64267\n",
      "step 480, dev loss 0.107733\n",
      "step 480, dev accuracy 0.625654\n",
      "step 496, dev loss 0.107116\n",
      "step 496, dev accuracy 0.640052\n",
      "step 512, dev loss 0.107522\n",
      "step 512, dev accuracy 0.626963\n",
      "step 528, dev loss 0.105083\n",
      "step 528, dev accuracy 0.638089\n",
      "step 544, dev loss 0.106421\n",
      "step 544, dev accuracy 0.638743\n",
      "step 560, dev loss 0.105687\n",
      "step 560, dev accuracy 0.643979\n",
      "step 576, dev loss 0.103587\n",
      "step 576, dev accuracy 0.646597\n",
      "step 592, dev loss 0.104444\n",
      "step 592, dev accuracy 0.645288\n",
      "step 608, dev loss 0.102593\n",
      "step 608, dev accuracy 0.657723\n",
      "step 624, dev loss 0.103941\n",
      "step 624, dev accuracy 0.661649\n",
      "step 640, dev loss 0.102764\n",
      "step 640, dev accuracy 0.657723\n",
      "step 656, dev loss 0.102754\n",
      "step 656, dev accuracy 0.662304\n",
      "step 672, dev loss 0.100254\n",
      "step 672, dev accuracy 0.67801\n",
      "\n",
      "Epoch 3\n",
      "----------\n",
      "step 688, dev loss 0.10184\n",
      "step 688, dev accuracy 0.682592\n",
      "step 704, dev loss 0.100409\n",
      "step 704, dev accuracy 0.674084\n",
      "step 720, dev loss 0.100455\n",
      "step 720, dev accuracy 0.674084\n",
      "step 736, dev loss 0.0995897\n",
      "step 736, dev accuracy 0.666885\n",
      "step 752, dev loss 0.0987584\n",
      "step 752, dev accuracy 0.693063\n",
      "step 768, dev loss 0.0967322\n",
      "step 768, dev accuracy 0.685209\n",
      "step 784, dev loss 0.102461\n",
      "step 784, dev accuracy 0.674738\n",
      "step 800, dev loss 0.102112\n",
      "step 800, dev accuracy 0.670157\n",
      "step 816, dev loss 0.0998271\n",
      "step 816, dev accuracy 0.690445\n",
      "step 832, dev loss 0.097418\n",
      "step 832, dev accuracy 0.687827\n",
      "step 848, dev loss 0.0992019\n",
      "step 848, dev accuracy 0.684555\n",
      "step 864, dev loss 0.100011\n",
      "step 864, dev accuracy 0.693717\n",
      "step 880, dev loss 0.098733\n",
      "step 880, dev accuracy 0.689791\n",
      "step 896, dev loss 0.0963602\n",
      "step 896, dev accuracy 0.704188\n",
      "\n",
      "Epoch 4\n",
      "----------\n",
      "step 912, dev loss 0.0975956\n",
      "step 912, dev accuracy 0.690445\n",
      "step 928, dev loss 0.0966206\n",
      "step 928, dev accuracy 0.695681\n",
      "step 944, dev loss 0.0955605\n",
      "step 944, dev accuracy 0.706806\n",
      "step 960, dev loss 0.09716\n",
      "step 960, dev accuracy 0.692408\n",
      "step 976, dev loss 0.0998138\n",
      "step 976, dev accuracy 0.697644\n",
      "step 992, dev loss 0.0946512\n",
      "step 992, dev accuracy 0.704188\n",
      "step 1008, dev loss 0.0990025\n",
      "step 1008, dev accuracy 0.691754\n",
      "step 1024, dev loss 0.0948127\n",
      "step 1024, dev accuracy 0.708115\n",
      "step 1040, dev loss 0.0932904\n",
      "step 1040, dev accuracy 0.713351\n",
      "step 1056, dev loss 0.095127\n",
      "step 1056, dev accuracy 0.707461\n",
      "step 1072, dev loss 0.0971341\n",
      "step 1072, dev accuracy 0.698953\n",
      "step 1088, dev loss 0.0952903\n",
      "step 1088, dev accuracy 0.713351\n",
      "step 1104, dev loss 0.0952058\n",
      "step 1104, dev accuracy 0.706806\n",
      "step 1120, dev loss 0.094315\n",
      "step 1120, dev accuracy 0.70877\n",
      "\n",
      "Epoch 5\n",
      "----------\n",
      "step 1136, dev loss 0.092708\n",
      "step 1136, dev accuracy 0.723822\n",
      "step 1152, dev loss 0.0915845\n",
      "step 1152, dev accuracy 0.731021\n",
      "step 1168, dev loss 0.0922868\n",
      "step 1168, dev accuracy 0.705497\n",
      "step 1184, dev loss 0.0903058\n",
      "step 1184, dev accuracy 0.723822\n",
      "step 1200, dev loss 0.0916083\n",
      "step 1200, dev accuracy 0.716623\n",
      "step 1216, dev loss 0.0908744\n",
      "step 1216, dev accuracy 0.72644\n",
      "step 1232, dev loss 0.0921867\n",
      "step 1232, dev accuracy 0.727094\n",
      "step 1248, dev loss 0.0899607\n",
      "step 1248, dev accuracy 0.73822\n",
      "step 1264, dev loss 0.0903669\n",
      "step 1264, dev accuracy 0.735602\n",
      "step 1280, dev loss 0.0918664\n",
      "step 1280, dev accuracy 0.710733\n",
      "step 1296, dev loss 0.0913218\n",
      "step 1296, dev accuracy 0.723822\n",
      "step 1312, dev loss 0.0928979\n",
      "step 1312, dev accuracy 0.719895\n",
      "step 1328, dev loss 0.08859\n",
      "step 1328, dev accuracy 0.731021\n",
      "step 1344, dev loss 0.0903583\n",
      "step 1344, dev accuracy 0.725785\n",
      "step 1360, dev loss 0.0880021\n",
      "step 1360, dev accuracy 0.73233\n",
      "\n",
      "Epoch 6\n",
      "----------\n",
      "step 1376, dev loss 0.0882803\n",
      "step 1376, dev accuracy 0.739529\n",
      "step 1392, dev loss 0.08953\n",
      "step 1392, dev accuracy 0.735602\n",
      "step 1408, dev loss 0.087105\n",
      "step 1408, dev accuracy 0.75\n",
      "step 1424, dev loss 0.0889041\n",
      "step 1424, dev accuracy 0.729712\n",
      "step 1440, dev loss 0.0892168\n",
      "step 1440, dev accuracy 0.732984\n",
      "step 1456, dev loss 0.0874902\n",
      "step 1456, dev accuracy 0.732984\n",
      "step 1472, dev loss 0.0876485\n",
      "step 1472, dev accuracy 0.74411\n",
      "step 1488, dev loss 0.0919033\n",
      "step 1488, dev accuracy 0.719241\n",
      "step 1504, dev loss 0.0889129\n",
      "step 1504, dev accuracy 0.734948\n",
      "step 1520, dev loss 0.0904265\n",
      "step 1520, dev accuracy 0.725785\n",
      "step 1536, dev loss 0.0873305\n",
      "step 1536, dev accuracy 0.75\n",
      "step 1552, dev loss 0.0883282\n",
      "step 1552, dev accuracy 0.735602\n",
      "step 1568, dev loss 0.0895726\n",
      "step 1568, dev accuracy 0.737565\n",
      "step 1584, dev loss 0.0889146\n",
      "step 1584, dev accuracy 0.735602\n",
      "\n",
      "Epoch 7\n",
      "----------\n",
      "step 1600, dev loss 0.0864668\n",
      "step 1600, dev accuracy 0.742147\n",
      "step 1616, dev loss 0.087864\n",
      "step 1616, dev accuracy 0.734293\n",
      "step 1632, dev loss 0.0876509\n",
      "step 1632, dev accuracy 0.735602\n",
      "step 1648, dev loss 0.0868452\n",
      "step 1648, dev accuracy 0.747382\n",
      "step 1664, dev loss 0.0840297\n",
      "step 1664, dev accuracy 0.742147\n",
      "step 1680, dev loss 0.0850708\n",
      "step 1680, dev accuracy 0.746728\n",
      "step 1696, dev loss 0.0855417\n",
      "step 1696, dev accuracy 0.73822\n",
      "step 1712, dev loss 0.0869525\n",
      "step 1712, dev accuracy 0.737565\n",
      "step 1728, dev loss 0.0898073\n",
      "step 1728, dev accuracy 0.736257\n",
      "step 1744, dev loss 0.0871772\n",
      "step 1744, dev accuracy 0.742801\n",
      "step 1760, dev loss 0.0844504\n",
      "step 1760, dev accuracy 0.748691\n",
      "step 1776, dev loss 0.0845392\n",
      "step 1776, dev accuracy 0.753272\n",
      "step 1792, dev loss 0.0875038\n",
      "step 1792, dev accuracy 0.737565\n",
      "step 1808, dev loss 0.087041\n",
      "step 1808, dev accuracy 0.734948\n",
      "\n",
      "Epoch 8\n",
      "----------\n",
      "step 1824, dev loss 0.0876579\n",
      "step 1824, dev accuracy 0.741492\n",
      "step 1840, dev loss 0.0824557\n",
      "step 1840, dev accuracy 0.759162\n",
      "step 1856, dev loss 0.0808586\n",
      "step 1856, dev accuracy 0.768325\n",
      "step 1872, dev loss 0.0840001\n",
      "step 1872, dev accuracy 0.751309\n",
      "step 1888, dev loss 0.084402\n",
      "step 1888, dev accuracy 0.757853\n",
      "step 1904, dev loss 0.0824519\n",
      "step 1904, dev accuracy 0.759162\n",
      "step 1920, dev loss 0.0866664\n",
      "step 1920, dev accuracy 0.748037\n",
      "step 1936, dev loss 0.0853789\n",
      "step 1936, dev accuracy 0.756545\n",
      "step 1952, dev loss 0.083647\n",
      "step 1952, dev accuracy 0.753927\n",
      "step 1968, dev loss 0.0824513\n",
      "step 1968, dev accuracy 0.761126\n",
      "step 1984, dev loss 0.0804694\n",
      "step 1984, dev accuracy 0.77356\n",
      "step 2000, dev loss 0.0835231\n",
      "step 2000, dev accuracy 0.770942\n",
      "step 2016, dev loss 0.0826903\n",
      "step 2016, dev accuracy 0.759817\n",
      "step 2032, dev loss 0.0820093\n",
      "step 2032, dev accuracy 0.75589\n",
      "\n",
      "Epoch 9\n",
      "----------\n",
      "step 2048, dev loss 0.0826794\n",
      "step 2048, dev accuracy 0.763743\n",
      "step 2064, dev loss 0.0839743\n",
      "step 2064, dev accuracy 0.754581\n",
      "step 2080, dev loss 0.0821852\n",
      "step 2080, dev accuracy 0.751309\n",
      "step 2096, dev loss 0.0833695\n",
      "step 2096, dev accuracy 0.751309\n",
      "step 2112, dev loss 0.0815678\n",
      "step 2112, dev accuracy 0.778796\n",
      "step 2128, dev loss 0.0793988\n",
      "step 2128, dev accuracy 0.775524\n",
      "step 2144, dev loss 0.0815677\n",
      "step 2144, dev accuracy 0.769634\n",
      "step 2160, dev loss 0.0821023\n",
      "step 2160, dev accuracy 0.75589\n",
      "step 2176, dev loss 0.080751\n",
      "step 2176, dev accuracy 0.770942\n",
      "step 2192, dev loss 0.081674\n",
      "step 2192, dev accuracy 0.762435\n",
      "step 2208, dev loss 0.0822008\n",
      "step 2208, dev accuracy 0.770288\n",
      "step 2224, dev loss 0.0795782\n",
      "step 2224, dev accuracy 0.774869\n",
      "step 2240, dev loss 0.0828386\n",
      "step 2240, dev accuracy 0.76767\n",
      "step 2256, dev loss 0.083803\n",
      "step 2256, dev accuracy 0.752618\n",
      "\n",
      "Epoch 10\n",
      "----------\n",
      "step 2272, dev loss 0.0826497\n",
      "step 2272, dev accuracy 0.757853\n",
      "step 2288, dev loss 0.0797977\n",
      "step 2288, dev accuracy 0.770288\n",
      "step 2304, dev loss 0.0839304\n",
      "step 2304, dev accuracy 0.760471\n",
      "step 2320, dev loss 0.083071\n",
      "step 2320, dev accuracy 0.766361\n",
      "step 2336, dev loss 0.080488\n",
      "step 2336, dev accuracy 0.760471\n",
      "step 2352, dev loss 0.0807566\n",
      "step 2352, dev accuracy 0.771597\n",
      "step 2368, dev loss 0.0802729\n",
      "step 2368, dev accuracy 0.764398\n",
      "step 2384, dev loss 0.0811257\n",
      "step 2384, dev accuracy 0.76178\n",
      "step 2400, dev loss 0.0814334\n",
      "step 2400, dev accuracy 0.771597\n",
      "step 2416, dev loss 0.0827979\n",
      "step 2416, dev accuracy 0.774215\n",
      "step 2432, dev loss 0.0807209\n",
      "step 2432, dev accuracy 0.765707\n",
      "step 2448, dev loss 0.0824244\n",
      "step 2448, dev accuracy 0.768979\n",
      "step 2464, dev loss 0.0796029\n",
      "step 2464, dev accuracy 0.777487\n",
      "step 2480, dev loss 0.0817107\n",
      "step 2480, dev accuracy 0.770288\n",
      "step 2496, dev loss 0.0802957\n",
      "step 2496, dev accuracy 0.771597\n",
      "\n",
      "Epoch 11\n",
      "----------\n",
      "step 2512, dev loss 0.082542\n",
      "step 2512, dev accuracy 0.768979\n",
      "step 2528, dev loss 0.079931\n",
      "step 2528, dev accuracy 0.776178\n",
      "step 2544, dev loss 0.0783765\n",
      "step 2544, dev accuracy 0.784686\n",
      "step 2560, dev loss 0.0784792\n",
      "step 2560, dev accuracy 0.78534\n",
      "step 2576, dev loss 0.0799406\n",
      "step 2576, dev accuracy 0.771597\n",
      "step 2592, dev loss 0.0773502\n",
      "step 2592, dev accuracy 0.785995\n",
      "step 2608, dev loss 0.0801058\n",
      "step 2608, dev accuracy 0.776178\n",
      "step 2624, dev loss 0.080793\n",
      "step 2624, dev accuracy 0.771597\n",
      "step 2640, dev loss 0.0798272\n",
      "step 2640, dev accuracy 0.77945\n",
      "step 2656, dev loss 0.0787697\n",
      "step 2656, dev accuracy 0.78534\n",
      "step 2672, dev loss 0.0783259\n",
      "step 2672, dev accuracy 0.77356\n",
      "step 2688, dev loss 0.0754235\n",
      "step 2688, dev accuracy 0.782068\n",
      "step 2704, dev loss 0.0796698\n",
      "step 2704, dev accuracy 0.781414\n",
      "step 2720, dev loss 0.0810196\n",
      "step 2720, dev accuracy 0.777487\n",
      "\n",
      "Epoch 12\n",
      "----------\n",
      "step 2736, dev loss 0.0792456\n",
      "step 2736, dev accuracy 0.782723\n",
      "step 2752, dev loss 0.0799012\n",
      "step 2752, dev accuracy 0.784686\n",
      "step 2768, dev loss 0.0770927\n",
      "step 2768, dev accuracy 0.788613\n",
      "step 2784, dev loss 0.0773599\n",
      "step 2784, dev accuracy 0.780759\n",
      "step 2800, dev loss 0.0766775\n",
      "step 2800, dev accuracy 0.775524\n",
      "step 2816, dev loss 0.0777504\n",
      "step 2816, dev accuracy 0.78534\n",
      "step 2832, dev loss 0.0805712\n",
      "step 2832, dev accuracy 0.776832\n",
      "step 2848, dev loss 0.0780592\n",
      "step 2848, dev accuracy 0.784686\n",
      "step 2864, dev loss 0.0806781\n",
      "step 2864, dev accuracy 0.768325\n",
      "step 2880, dev loss 0.0767765\n",
      "step 2880, dev accuracy 0.795812\n",
      "step 2896, dev loss 0.0780169\n",
      "step 2896, dev accuracy 0.778796\n",
      "step 2912, dev loss 0.078824\n",
      "step 2912, dev accuracy 0.780759\n",
      "step 2928, dev loss 0.0806627\n",
      "step 2928, dev accuracy 0.778141\n",
      "step 2944, dev loss 0.0794263\n",
      "step 2944, dev accuracy 0.76767\n",
      "\n",
      "Epoch 13\n",
      "----------\n",
      "step 2960, dev loss 0.0773247\n",
      "step 2960, dev accuracy 0.783377\n",
      "step 2976, dev loss 0.0781512\n",
      "step 2976, dev accuracy 0.784686\n",
      "step 2992, dev loss 0.0792905\n",
      "step 2992, dev accuracy 0.787304\n",
      "step 3008, dev loss 0.0778338\n",
      "step 3008, dev accuracy 0.784686\n",
      "step 3024, dev loss 0.0771898\n",
      "step 3024, dev accuracy 0.784031\n",
      "step 3040, dev loss 0.0730963\n",
      "step 3040, dev accuracy 0.790576\n",
      "step 3056, dev loss 0.0771467\n",
      "step 3056, dev accuracy 0.79123\n",
      "step 3072, dev loss 0.0789809\n",
      "step 3072, dev accuracy 0.781414\n",
      "step 3088, dev loss 0.0780623\n",
      "step 3088, dev accuracy 0.782723\n",
      "step 3104, dev loss 0.0788385\n",
      "step 3104, dev accuracy 0.778141\n",
      "step 3120, dev loss 0.0789327\n",
      "step 3120, dev accuracy 0.782068\n",
      "step 3136, dev loss 0.0803519\n",
      "step 3136, dev accuracy 0.789267\n",
      "step 3152, dev loss 0.0780216\n",
      "step 3152, dev accuracy 0.787304\n",
      "step 3168, dev loss 0.0781742\n",
      "step 3168, dev accuracy 0.778796\n",
      "\n",
      "Epoch 14\n",
      "----------\n",
      "step 3184, dev loss 0.0769802\n",
      "step 3184, dev accuracy 0.792539\n",
      "step 3200, dev loss 0.0814334\n",
      "step 3200, dev accuracy 0.776178\n",
      "step 3216, dev loss 0.0775607\n",
      "step 3216, dev accuracy 0.786649\n",
      "step 3232, dev loss 0.0757606\n",
      "step 3232, dev accuracy 0.789921\n",
      "step 3248, dev loss 0.0766372\n",
      "step 3248, dev accuracy 0.776832\n",
      "step 3264, dev loss 0.0788991\n",
      "step 3264, dev accuracy 0.783377\n",
      "step 3280, dev loss 0.0755151\n",
      "step 3280, dev accuracy 0.79712\n",
      "step 3296, dev loss 0.0778848\n",
      "step 3296, dev accuracy 0.774869\n",
      "step 3312, dev loss 0.0752269\n",
      "step 3312, dev accuracy 0.797775\n",
      "step 3328, dev loss 0.0774459\n",
      "step 3328, dev accuracy 0.778796\n",
      "step 3344, dev loss 0.0784535\n",
      "step 3344, dev accuracy 0.781414\n",
      "step 3360, dev loss 0.0753278\n",
      "step 3360, dev accuracy 0.784031\n",
      "step 3376, dev loss 0.0788641\n",
      "step 3376, dev accuracy 0.780105\n",
      "step 3392, dev loss 0.0806202\n",
      "step 3392, dev accuracy 0.785995\n",
      "\n",
      "Epoch 15\n",
      "----------\n",
      "step 3408, dev loss 0.0760407\n",
      "step 3408, dev accuracy 0.789267\n",
      "step 3424, dev loss 0.0744262\n",
      "step 3424, dev accuracy 0.793194\n",
      "step 3440, dev loss 0.0758751\n",
      "step 3440, dev accuracy 0.795157\n",
      "step 3456, dev loss 0.0783771\n",
      "step 3456, dev accuracy 0.78534\n",
      "step 3472, dev loss 0.0781106\n",
      "step 3472, dev accuracy 0.790576\n",
      "step 3488, dev loss 0.0767769\n",
      "step 3488, dev accuracy 0.793848\n",
      "step 3504, dev loss 0.0766013\n",
      "step 3504, dev accuracy 0.786649\n",
      "step 3520, dev loss 0.0745137\n",
      "step 3520, dev accuracy 0.796466\n",
      "step 3536, dev loss 0.0752493\n",
      "step 3536, dev accuracy 0.792539\n",
      "step 3552, dev loss 0.0771985\n",
      "step 3552, dev accuracy 0.797775\n",
      "step 3568, dev loss 0.0739829\n",
      "step 3568, dev accuracy 0.818717\n",
      "step 3584, dev loss 0.0776\n",
      "step 3584, dev accuracy 0.797775\n",
      "step 3600, dev loss 0.0747435\n",
      "step 3600, dev accuracy 0.800393\n",
      "step 3616, dev loss 0.0769483\n",
      "step 3616, dev accuracy 0.793194\n",
      "\n",
      "Epoch 16\n",
      "----------\n",
      "step 3632, dev loss 0.0761946\n",
      "step 3632, dev accuracy 0.787958\n",
      "step 3648, dev loss 0.0765326\n",
      "step 3648, dev accuracy 0.793848\n",
      "step 3664, dev loss 0.0779063\n",
      "step 3664, dev accuracy 0.782723\n",
      "step 3680, dev loss 0.0730874\n",
      "step 3680, dev accuracy 0.794503\n",
      "step 3696, dev loss 0.075955\n",
      "step 3696, dev accuracy 0.78534\n",
      "step 3712, dev loss 0.0740595\n",
      "step 3712, dev accuracy 0.798429\n",
      "step 3728, dev loss 0.0773164\n",
      "step 3728, dev accuracy 0.784031\n",
      "step 3744, dev loss 0.0773647\n",
      "step 3744, dev accuracy 0.794503\n",
      "step 3760, dev loss 0.0782061\n",
      "step 3760, dev accuracy 0.79712\n",
      "step 3776, dev loss 0.0763976\n",
      "step 3776, dev accuracy 0.783377\n",
      "step 3792, dev loss 0.0774836\n",
      "step 3792, dev accuracy 0.789267\n",
      "step 3808, dev loss 0.0749469\n",
      "step 3808, dev accuracy 0.790576\n",
      "step 3824, dev loss 0.0743589\n",
      "step 3824, dev accuracy 0.788613\n",
      "step 3840, dev loss 0.0758807\n",
      "step 3840, dev accuracy 0.795812\n",
      "step 3856, dev loss 0.0788264\n",
      "step 3856, dev accuracy 0.790576\n",
      "\n",
      "Epoch 17\n",
      "----------\n",
      "step 3872, dev loss 0.0753343\n",
      "step 3872, dev accuracy 0.793848\n",
      "step 3888, dev loss 0.0794066\n",
      "step 3888, dev accuracy 0.789267\n",
      "step 3904, dev loss 0.0762509\n",
      "step 3904, dev accuracy 0.788613\n",
      "step 3920, dev loss 0.0741405\n",
      "step 3920, dev accuracy 0.800393\n",
      "step 3936, dev loss 0.0758917\n",
      "step 3936, dev accuracy 0.799738\n",
      "step 3952, dev loss 0.0711677\n",
      "step 3952, dev accuracy 0.808246\n",
      "step 3968, dev loss 0.074079\n",
      "step 3968, dev accuracy 0.802356\n",
      "step 3984, dev loss 0.0777601\n",
      "step 3984, dev accuracy 0.795812\n",
      "step 4000, dev loss 0.0760194\n",
      "step 4000, dev accuracy 0.788613\n",
      "step 4016, dev loss 0.0744269\n",
      "step 4016, dev accuracy 0.79712\n",
      "step 4032, dev loss 0.0743651\n",
      "step 4032, dev accuracy 0.801702\n",
      "step 4048, dev loss 0.078519\n",
      "step 4048, dev accuracy 0.788613\n",
      "step 4064, dev loss 0.0757636\n",
      "step 4064, dev accuracy 0.795157\n",
      "step 4080, dev loss 0.0763658\n",
      "step 4080, dev accuracy 0.795157\n",
      "\n",
      "Epoch 18\n",
      "----------\n",
      "step 4096, dev loss 0.0720743\n",
      "step 4096, dev accuracy 0.799738\n",
      "step 4112, dev loss 0.0759342\n",
      "step 4112, dev accuracy 0.799738\n",
      "step 4128, dev loss 0.0727898\n",
      "step 4128, dev accuracy 0.804319\n",
      "step 4144, dev loss 0.0761541\n",
      "step 4144, dev accuracy 0.797775\n",
      "step 4160, dev loss 0.0775939\n",
      "step 4160, dev accuracy 0.784031\n",
      "step 4176, dev loss 0.074622\n",
      "step 4176, dev accuracy 0.794503\n",
      "step 4192, dev loss 0.0754017\n",
      "step 4192, dev accuracy 0.804974\n",
      "step 4208, dev loss 0.0742329\n",
      "step 4208, dev accuracy 0.796466\n",
      "step 4224, dev loss 0.0755348\n",
      "step 4224, dev accuracy 0.799738\n",
      "step 4240, dev loss 0.076953\n",
      "step 4240, dev accuracy 0.796466\n",
      "step 4256, dev loss 0.0789823\n",
      "step 4256, dev accuracy 0.79123\n",
      "step 4272, dev loss 0.0721483\n",
      "step 4272, dev accuracy 0.812827\n",
      "step 4288, dev loss 0.0772846\n",
      "step 4288, dev accuracy 0.793194\n",
      "step 4304, dev loss 0.0766916\n",
      "step 4304, dev accuracy 0.799084\n",
      "\n",
      "Epoch 19\n",
      "----------\n",
      "step 4320, dev loss 0.0751918\n",
      "step 4320, dev accuracy 0.799084\n",
      "step 4336, dev loss 0.0757925\n",
      "step 4336, dev accuracy 0.796466\n",
      "step 4352, dev loss 0.0747402\n",
      "step 4352, dev accuracy 0.801047\n",
      "step 4368, dev loss 0.0755399\n",
      "step 4368, dev accuracy 0.808901\n",
      "step 4384, dev loss 0.0761253\n",
      "step 4384, dev accuracy 0.808246\n",
      "step 4400, dev loss 0.075435\n",
      "step 4400, dev accuracy 0.793194\n",
      "step 4416, dev loss 0.0786854\n",
      "step 4416, dev accuracy 0.782068\n",
      "step 4432, dev loss 0.0785509\n",
      "step 4432, dev accuracy 0.795157\n",
      "step 4448, dev loss 0.0740202\n",
      "step 4448, dev accuracy 0.804319\n",
      "step 4464, dev loss 0.0740587\n",
      "step 4464, dev accuracy 0.810209\n",
      "step 4480, dev loss 0.0769068\n",
      "step 4480, dev accuracy 0.799084\n",
      "step 4496, dev loss 0.076904\n",
      "step 4496, dev accuracy 0.793848\n",
      "step 4512, dev loss 0.0772458\n",
      "step 4512, dev accuracy 0.793194\n",
      "step 4528, dev loss 0.0734837\n",
      "step 4528, dev accuracy 0.814791\n",
      "\n",
      "Epoch 20\n",
      "----------\n",
      "step 4544, dev loss 0.0781355\n",
      "step 4544, dev accuracy 0.790576\n",
      "step 4560, dev loss 0.0747174\n",
      "step 4560, dev accuracy 0.808246\n",
      "step 4576, dev loss 0.0766436\n",
      "step 4576, dev accuracy 0.801702\n",
      "step 4592, dev loss 0.0785601\n",
      "step 4592, dev accuracy 0.798429\n",
      "step 4608, dev loss 0.0779104\n",
      "step 4608, dev accuracy 0.799738\n",
      "step 4624, dev loss 0.0737447\n",
      "step 4624, dev accuracy 0.803665\n",
      "step 4640, dev loss 0.0727055\n",
      "step 4640, dev accuracy 0.817408\n",
      "step 4656, dev loss 0.0764062\n",
      "step 4656, dev accuracy 0.798429\n",
      "step 4672, dev loss 0.0763121\n",
      "step 4672, dev accuracy 0.798429\n",
      "step 4688, dev loss 0.0748134\n",
      "step 4688, dev accuracy 0.798429\n",
      "step 4704, dev loss 0.0769116\n",
      "step 4704, dev accuracy 0.801047\n",
      "step 4720, dev loss 0.0759482\n",
      "step 4720, dev accuracy 0.793848\n",
      "step 4736, dev loss 0.0769309\n",
      "step 4736, dev accuracy 0.793848\n",
      "step 4752, dev loss 0.0807649\n",
      "step 4752, dev accuracy 0.79123\n",
      "\n",
      "Epoch 21\n",
      "----------\n",
      "step 4768, dev loss 0.0764467\n",
      "step 4768, dev accuracy 0.794503\n",
      "step 4784, dev loss 0.0750743\n",
      "step 4784, dev accuracy 0.802356\n",
      "step 4800, dev loss 0.0721823\n",
      "step 4800, dev accuracy 0.806937\n",
      "step 4816, dev loss 0.0756146\n",
      "step 4816, dev accuracy 0.804974\n",
      "step 4832, dev loss 0.0785539\n",
      "step 4832, dev accuracy 0.801702\n",
      "step 4848, dev loss 0.0757817\n",
      "step 4848, dev accuracy 0.804974\n",
      "step 4864, dev loss 0.0753672\n",
      "step 4864, dev accuracy 0.805628\n",
      "step 4880, dev loss 0.0755878\n",
      "step 4880, dev accuracy 0.797775\n",
      "step 4896, dev loss 0.0727327\n",
      "step 4896, dev accuracy 0.810864\n",
      "step 4912, dev loss 0.0730695\n",
      "step 4912, dev accuracy 0.812173\n",
      "step 4928, dev loss 0.0784306\n",
      "step 4928, dev accuracy 0.799738\n",
      "step 4944, dev loss 0.0787894\n",
      "step 4944, dev accuracy 0.796466\n",
      "step 4960, dev loss 0.0753793\n",
      "step 4960, dev accuracy 0.798429\n",
      "step 4976, dev loss 0.0748399\n",
      "step 4976, dev accuracy 0.804974\n",
      "step 4992, dev loss 0.0762756\n",
      "step 4992, dev accuracy 0.809555\n",
      "\n",
      "Epoch 22\n",
      "----------\n",
      "step 5008, dev loss 0.0713516\n",
      "step 5008, dev accuracy 0.808246\n",
      "step 5024, dev loss 0.0785088\n",
      "step 5024, dev accuracy 0.795812\n",
      "step 5040, dev loss 0.0745563\n",
      "step 5040, dev accuracy 0.813482\n",
      "step 5056, dev loss 0.0746873\n",
      "step 5056, dev accuracy 0.808901\n",
      "step 5072, dev loss 0.0788522\n",
      "step 5072, dev accuracy 0.799738\n",
      "step 5088, dev loss 0.0736111\n",
      "step 5088, dev accuracy 0.812827\n",
      "step 5104, dev loss 0.0793972\n",
      "step 5104, dev accuracy 0.801702\n",
      "step 5120, dev loss 0.0780406\n",
      "step 5120, dev accuracy 0.814136\n",
      "step 5136, dev loss 0.076548\n",
      "step 5136, dev accuracy 0.805628\n",
      "step 5152, dev loss 0.0759687\n",
      "step 5152, dev accuracy 0.806283\n",
      "step 5168, dev loss 0.0759514\n",
      "step 5168, dev accuracy 0.799084\n",
      "step 5184, dev loss 0.080676\n",
      "step 5184, dev accuracy 0.795812\n",
      "step 5200, dev loss 0.0778107\n",
      "step 5200, dev accuracy 0.807592\n",
      "step 5216, dev loss 0.0758338\n",
      "step 5216, dev accuracy 0.815445\n",
      "\n",
      "Epoch 23\n",
      "----------\n",
      "step 5232, dev loss 0.0754422\n",
      "step 5232, dev accuracy 0.793848\n",
      "step 5248, dev loss 0.074551\n",
      "step 5248, dev accuracy 0.805628\n",
      "step 5264, dev loss 0.0766048\n",
      "step 5264, dev accuracy 0.810209\n",
      "step 5280, dev loss 0.0796078\n",
      "step 5280, dev accuracy 0.806283\n",
      "step 5296, dev loss 0.0786716\n",
      "step 5296, dev accuracy 0.803665\n",
      "step 5312, dev loss 0.0756938\n",
      "step 5312, dev accuracy 0.811518\n",
      "step 5328, dev loss 0.0812185\n",
      "step 5328, dev accuracy 0.792539\n",
      "step 5344, dev loss 0.0811481\n",
      "step 5344, dev accuracy 0.797775\n",
      "step 5360, dev loss 0.0755203\n",
      "step 5360, dev accuracy 0.79712\n",
      "step 5376, dev loss 0.0812921\n",
      "step 5376, dev accuracy 0.795812\n",
      "step 5392, dev loss 0.0785725\n",
      "step 5392, dev accuracy 0.791885\n",
      "step 5408, dev loss 0.0784881\n",
      "step 5408, dev accuracy 0.795157\n",
      "step 5424, dev loss 0.0817241\n",
      "step 5424, dev accuracy 0.80301\n",
      "step 5440, dev loss 0.0763877\n",
      "step 5440, dev accuracy 0.806937\n",
      "\n",
      "Epoch 24\n",
      "----------\n",
      "step 5456, dev loss 0.0753803\n",
      "step 5456, dev accuracy 0.799738\n",
      "step 5472, dev loss 0.0741021\n",
      "step 5472, dev accuracy 0.805628\n",
      "step 5488, dev loss 0.0755123\n",
      "step 5488, dev accuracy 0.794503\n",
      "step 5504, dev loss 0.0764457\n",
      "step 5504, dev accuracy 0.799084\n",
      "step 5520, dev loss 0.0828509\n",
      "step 5520, dev accuracy 0.787958\n",
      "step 5536, dev loss 0.0785451\n",
      "step 5536, dev accuracy 0.793848\n",
      "step 5552, dev loss 0.0790108\n",
      "step 5552, dev accuracy 0.801702\n",
      "step 5568, dev loss 0.0838172\n",
      "step 5568, dev accuracy 0.801047\n",
      "step 5584, dev loss 0.0801304\n",
      "step 5584, dev accuracy 0.810864\n",
      "step 5600, dev loss 0.0806425\n",
      "step 5600, dev accuracy 0.804974\n",
      "step 5616, dev loss 0.0767691\n",
      "step 5616, dev accuracy 0.808246\n",
      "step 5632, dev loss 0.0770654\n",
      "step 5632, dev accuracy 0.812173\n",
      "step 5648, dev loss 0.0766705\n",
      "step 5648, dev accuracy 0.804974\n",
      "step 5664, dev loss 0.0775102\n",
      "step 5664, dev accuracy 0.812827\n",
      "\n",
      "Epoch 25\n",
      "----------\n",
      "step 5680, dev loss 0.07947\n",
      "step 5680, dev accuracy 0.811518\n",
      "step 5696, dev loss 0.0767701\n",
      "step 5696, dev accuracy 0.804319\n",
      "step 5712, dev loss 0.0801056\n",
      "step 5712, dev accuracy 0.808246\n",
      "step 5728, dev loss 0.0789177\n",
      "step 5728, dev accuracy 0.799738\n",
      "step 5744, dev loss 0.0765218\n",
      "step 5744, dev accuracy 0.810864\n",
      "step 5760, dev loss 0.0767965\n",
      "step 5760, dev accuracy 0.80301\n",
      "step 5776, dev loss 0.0728988\n",
      "step 5776, dev accuracy 0.812173\n",
      "step 5792, dev loss 0.0782653\n",
      "step 5792, dev accuracy 0.805628\n",
      "step 5808, dev loss 0.0773217\n",
      "step 5808, dev accuracy 0.809555\n",
      "step 5824, dev loss 0.0775161\n",
      "step 5824, dev accuracy 0.810209\n",
      "step 5840, dev loss 0.0802127\n",
      "step 5840, dev accuracy 0.812173\n",
      "step 5856, dev loss 0.0772133\n",
      "step 5856, dev accuracy 0.810864\n",
      "step 5872, dev loss 0.0748316\n",
      "step 5872, dev accuracy 0.814791\n",
      "step 5888, dev loss 0.0816697\n",
      "step 5888, dev accuracy 0.795157\n",
      "\n",
      "Epoch 26\n",
      "----------\n",
      "step 5904, dev loss 0.0752522\n",
      "step 5904, dev accuracy 0.811518\n",
      "step 5920, dev loss 0.0767409\n",
      "step 5920, dev accuracy 0.806937\n",
      "step 5936, dev loss 0.0764589\n",
      "step 5936, dev accuracy 0.814136\n",
      "step 5952, dev loss 0.0815501\n",
      "step 5952, dev accuracy 0.80301\n",
      "step 5968, dev loss 0.0784238\n",
      "step 5968, dev accuracy 0.806283\n",
      "step 5984, dev loss 0.0780837\n",
      "step 5984, dev accuracy 0.806283\n",
      "step 6000, dev loss 0.0760622\n",
      "step 6000, dev accuracy 0.808246\n",
      "step 6016, dev loss 0.0831233\n",
      "step 6016, dev accuracy 0.802356\n",
      "step 6032, dev loss 0.0832531\n",
      "step 6032, dev accuracy 0.79712\n",
      "step 6048, dev loss 0.0755306\n",
      "step 6048, dev accuracy 0.816099\n",
      "step 6064, dev loss 0.0831243\n",
      "step 6064, dev accuracy 0.795812\n",
      "step 6080, dev loss 0.0789983\n",
      "step 6080, dev accuracy 0.809555\n",
      "step 6096, dev loss 0.0773294\n",
      "step 6096, dev accuracy 0.802356\n",
      "step 6112, dev loss 0.078155\n",
      "step 6112, dev accuracy 0.810864\n",
      "step 6128, dev loss 0.0776495\n",
      "step 6128, dev accuracy 0.799738\n",
      "\n",
      "Epoch 27\n",
      "----------\n",
      "step 6144, dev loss 0.0798085\n",
      "step 6144, dev accuracy 0.806283\n",
      "step 6160, dev loss 0.0846655\n",
      "step 6160, dev accuracy 0.795812\n",
      "step 6176, dev loss 0.0792499\n",
      "step 6176, dev accuracy 0.810864\n",
      "step 6192, dev loss 0.0799923\n",
      "step 6192, dev accuracy 0.807592\n",
      "step 6208, dev loss 0.0892726\n",
      "step 6208, dev accuracy 0.78534\n",
      "step 6224, dev loss 0.0774207\n",
      "step 6224, dev accuracy 0.808901\n",
      "step 6240, dev loss 0.0796737\n",
      "step 6240, dev accuracy 0.810209\n",
      "step 6256, dev loss 0.0844618\n",
      "step 6256, dev accuracy 0.795157\n",
      "step 6272, dev loss 0.083548\n",
      "step 6272, dev accuracy 0.803665\n",
      "step 6288, dev loss 0.0776247\n",
      "step 6288, dev accuracy 0.800393\n",
      "step 6304, dev loss 0.0838138\n",
      "step 6304, dev accuracy 0.813482\n",
      "step 6320, dev loss 0.0828098\n",
      "step 6320, dev accuracy 0.808901\n",
      "step 6336, dev loss 0.0785312\n",
      "step 6336, dev accuracy 0.815445\n",
      "step 6352, dev loss 0.0811052\n",
      "step 6352, dev accuracy 0.804974\n",
      "\n",
      "Epoch 28\n",
      "----------\n",
      "step 6368, dev loss 0.080701\n",
      "step 6368, dev accuracy 0.79712\n",
      "step 6384, dev loss 0.0763537\n",
      "step 6384, dev accuracy 0.821335\n",
      "step 6400, dev loss 0.0806231\n",
      "step 6400, dev accuracy 0.806283\n",
      "step 6416, dev loss 0.0808681\n",
      "step 6416, dev accuracy 0.802356\n",
      "step 6432, dev loss 0.0820926\n",
      "step 6432, dev accuracy 0.812173\n",
      "step 6448, dev loss 0.0813894\n",
      "step 6448, dev accuracy 0.803665\n",
      "step 6464, dev loss 0.0836157\n",
      "step 6464, dev accuracy 0.804974\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-208-e04d4195da10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCHECKPOINTS_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_batched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x_batched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_batched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mEVALUATE_DEV_EVERY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0mevaluate_dev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-207-92fa8104b570>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(batch_x, batch_y, step)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# print(\"step %d, train loss: %g\" % (step, loss))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/projects/python-venvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_fetch_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/projects/python-venvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m       return tf_session.TF_Run(self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 404\u001b[0;31m                                target_list)\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "shape_variables = []\n",
    "step = 0\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    build_graph()\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        # Print shapes\n",
    "        print_shapes(train_x_batched[0], train_y_batched[0])\n",
    "        # Initialize summary writers and savers\n",
    "        summary_writer_train = tf.train.SummaryWriter(TRAIN_SUMMARY_DIR, graph_def=sess.graph_def)\n",
    "        summary_writer_dev = tf.train.SummaryWriter(DEV_SUMMARY_DIR, graph_def=sess.graph_def)\n",
    "        saver = tf.train.Saver()\n",
    "        # For each epoch and batch...\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print(\"\\nEpoch %d\" % epoch)\n",
    "            print(\"----------\")\n",
    "            # Save each epoch\n",
    "            saver.save(sess, CHECKPOINTS_DIR, global_step=step)\n",
    "            for i in range(len(train_x_batched)):\n",
    "                train_batch(train_x_batched[i], train_y_batched[i], step)\n",
    "                if(step % EVALUATE_DEV_EVERY == 0):\n",
    "                    evaluate_dev(step)\n",
    "                step += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
